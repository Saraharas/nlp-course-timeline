#!/usr/bin/env python
# coding: utf-8

# # HMM

# Алгоритм Баума-Велша (Baum-Welch) - это алгоритм обучения скрытой марковской модели (HMM) при заданной последовательности наблюдаемых данных.
# 
# Алгоритм Baum-Welch является одним из ключевых методов для обучения параметров скрытой марковской модели (HMM) при наличии только наблюдаемых данных. Он является частью более общего алгоритма, известного как алгоритм Expectation-Maximization (EM), который используется для оценки параметров статистических моделей в случае, когда некоторые переменные (в данном случае, скрытые состояния) недоступны для наблюдения.
# 
# Суть алгоритма Baum-Welch заключается в том, чтобы находить наилучшие оценки параметров HMM, максимизируя правдоподобие наблюдаемых данных при условии модели. Он выполняет это обновлением параметров итеративно в два шага:
# 
# 1. **Expectation Step (E-шаг)**: На этом шаге алгоритм вычисляет ожидания скрытых состояний (посредством прямого и обратного проходов) для заданных наблюдений и текущих параметров модели.
# 
# 2. **Maximization Step (M-шаг)**: На этом шаге алгоритм обновляет параметры модели (начальные вероятности, матрицу переходов и матрицу эмиссий) так, чтобы максимизировать правдоподобие наблюдаемых данных, учитывая ожидания скрытых состояний, полученные на E-шаге.
# 
# Эти два шага выполняются итеративно до сходимости (или до достижения заданного числа итераций). В конечном итоге, Baum-Welch адаптирует параметры HMM к данным, что позволяет модели более точно описывать наблюдаемые последовательности.
# 
# Сценарии применения алгоритма Baum-Welch включают обучение HMM для ряда задач, таких как распознавание речи, обработка естественного языка, анализ временных рядов и другие, где присутствует скрытая структура и наблюдаем
# 
# Алгоритм Витерби является важным компонентом работы со скрытыми марковскими моделями (HMM) и имеет несколько важных применений:
# 
# 1. **Наилучший путь (Best Path) в HMM**: Основное применение алгоритма Витерби - это нахождение наиболее вероятной последовательности скрытых состояний в HMM для заданной последовательности наблюдаемых данных. Это полезно во многих задачах, таких как распознавание речи, где нужно определить, какие слова или фонемы наиболее вероятно соответствуют записи речи.
# 
# 2. **Выравнивание последовательностей**: Алгоритм Витерби может использоваться для выравнивания двух последовательностей, одна из которых является наблюдаемой, а другая - скрытой. Это применение находит свое применение в биоинформатике, где можно определять гомологичные участки геномов, выравнивая последовательности нуклеотидов.
# 
# 3. **Декодирование кодовых последовательностей**: Алгоритм Витерби используется в декодировании кодовых последовательностей, таких как коды Хэмминга, коды Хаффмана и другие. Он помогает определить наиболее вероятное исходное сообщение на основе полученных данных.
# 
# 4. **Сжатие данных**: В контексте сжатия данных, алгоритм Витерби используется для декодирования сжатых последовательностей. Например, в беспроводных связях, алгоритм Витерби может быть применен для восстановления данных, полученных с помощью кодирования с помощью сверточных кодов.
# 
# 5. **Разрешение амбигуитетов**: В различных областях, где есть амбигуитеты или неопределенности, алгоритм Витерби может помочь выбрать наилучший вариант. Например, в машинном переводе, где несколько переводов могут быть возможными, алгоритм Витерби может помочь выбрать наиболее `transition_matrix` и `emission_matrix` - это две важные матрицы, используемые в модели скрытой марковской модели (HMM) для моделирования последовательности наблюдений и скрытых состояний.
# 
# 1. **Матрица переходов (Transition Matrix):**
#    - `transition_matrix` представляет собой матрицу размера `num_states x num_states`, где `num_states` - это количество скрытых состояний в HMM.
#    - Каждый элемент `transition_matrix[i][j]` представляет вероятность перехода из скрытого состояния `i` в скрытое состояние `j`. То есть, она определяет вероятности переходов между скрытыми состояниями на каждом временном шаге.
#    - Например, `transition_matrix[0][1]` представляет вероятность перехода из состояния 0 в состояние 1.
# 
# 2. **Матрица эмиссий (Emission Matrix):**
#    - `emission_matrix` также является матрицей, размер которой `num_states x num_observations`. 
#    - Каждый элемент `emission_matrix[i][k]` представляет вероятность наблюдения `k` при нахождении системы в скрытом состоянии `i`. Это определяет вероятность того, какие наблюдения связаны с каждым из скрытых состояний.
#    - Например, `emission_matrix[1][2]` представляет вероятность наблюдения 2 (или другой символ, в зависимости от вашей задачи) при нахождении системы в состоянии 1.
# 
# Вместе `transition_matrix` и `emission_matrix` определяют модель HMM, которая используется для оценки вероятностей скрытых состояний и наблюдений во временных последовательностях. Эти матрицы обучаются с использованием алгоритма Баума-Велша на основе наблюдаемых данных.ерная лингвистика и мн
# 
# В контексте скрытой марковской модели (HMM) "скрытые состояния" и "наблюдения" - это ключевые концепции, которые используются для 
# моделирования и анализа последовательностей данных. Давайте объясним эти концепции на примере.
# 
# 1. **Скрытые состояния (Hidden States):**  
#    Скрытые состояния - это некие абстрактные состояния, которые не наблюдаются непосредственно, но влияют на наблюдаемые данные. 
#    Эти состояния могут быть интерпретированы как внутренние состояния системы, которые меняются со временем. В контексте HMM, 
#    скрытые состояния формируют марковскую цепь, то есть переходы между состояниями зависят только от текущего состояния и не зависят 
#    от предыдущих состояний. В вашем примере `num_states = 3`, это означает, что у вас есть 3 различных скрытых состояния, которые могут 
#    быть обозначены, например, как "Состояние 1", "Состояние 2" и "Состояние 3".
# 
# 2. **Наблюдения (Observations):**  
#    Наблюдения - это данные, которые мы непосредственно наблюдаем или измеряем. Они связаны с скрытыми состояниями, и каждое скрытое 
#    состояние может производить разные наблюдения с разными вероятностями. В контексте HMM, наблюдения могут быть категоризированы, 
#    и `num_observations = 4` означает, что у нас есть 4 возможных категории наблюдений. Например, если мы рассматриваем модель для 
#    текстовой последовательности, наблюдения могут представлять собой слова, и `num_observations` будет равно количеству различных 
#    слов в нашем словаре.
# 
# Давайте представим простой пример. Пусть у нас есть HMM для моделирования погоды:
# 
# - Скрытые состояния (HMM состояния): "Солнечно", "Пасмурно", "Дождь".
# - Наблюдения: "Зонт", "Очки", "Зонт", "Зонт", "Очки".
# 
# В этом примере, скрытые состояния представляют разные погодные условия, которые мы не наблюдаем напрямую, а наблюдения - это вещи, 
# которые мы можем видеть, например, носить зонт или солнечные очки. Задача HMM - определить, какие погодные условия (скрытые состояния) 
# наиболее вероятно привели к данным наблюдениям (вещам, которые мы видим).огие другие.ые данные.

# In[1]:


import numpy as np

class Baum_Welch:
    def __init__(self, num_states, num_observations):
        # Инициализация класса Baum_Welch с заданным количеством состояний и наблюдений.
        self.num_states = num_states  # Хранение количества скрытых состояний.
        self.num_observations = num_observations  # Хранение количества возможных наблюдений.
        
        # Инициализация начальных вероятностей состояний случайными значениями и их нормализация.
        self.initial_probabilities = np.random.rand(num_states)
        self.initial_probabilities /= np.sum(self.initial_probabilities)
        
        # Инициализация матрицы переходов между состояниями случайными значениями и их нормализация по строкам.
        self.transition_matrix = np.random.rand(num_states, num_states)
        self.transition_matrix /= np.sum(self.transition_matrix, axis=1, keepdims=True)
        
        # Инициализация матрицы эмиссий случайными значениями и их нормализация по строкам.
        self.emission_matrix = np.random.rand(num_states, num_observations)
        self.emission_matrix /= np.sum(self.emission_matrix, axis=1, keepdims=True)

    
    def forward(self, observations):
        # Прямой проход (forward pass) с использованием алгоритма Витерби
        T = len(observations)  # Количество наблюдений в последовательности
        alpha = np.zeros((T, self.num_states))  # Матрица альфа для хранения промежуточных значений
        
        # Инициализация первого шага алгоритма
        alpha[0] = self.initial_probabilities * self.emission_matrix[:, observations[0]]
        
        for t in range(1, T):
            for j in range(self.num_states):
                # Рекурсивное вычисление альфа для каждого состояния на текущем временном шаге
                alpha[t, j] = np.sum(alpha[t - 1] * self.transition_matrix[:, j]) * self.emission_matrix[j, observations[t]]
        
        return alpha  # Возвращение матрицы альфа, содержащей прямые проходы для всех временных шагов

    
    def backward(self, observations):
        # Обратный проход (backward pass)
        T = len(observations)  # Количество наблюдений в последовательности
        beta = np.zeros((T, self.num_states))  # Матрица бета для хранения промежуточных значений
        beta[-1] = 1.0  # Инициализация последнего шага обратного прохода
        
        # Обратный проход начиная с предпоследнего временного шага
        for t in range(T - 2, -1, -1):
            for i in range(self.num_states):
                # Рекурсивное вычисление бета для каждого состояния на текущем временном шаге
                beta[t, i] = np.sum(self.transition_matrix[i, :] * self.emission_matrix[:, observations[t + 1]] * beta[t + 1, :])
        
        return beta  # Возвращение матрицы бета, содержащей обратные проходы для всех временных шагов

    
    def train(self, observations, num_iterations=100):
        # Обучение параметров HMM с использованием алгоритма Баума-Велша
        for iteration in range(num_iterations):
            alpha = self.forward(observations)  # Прямой проход
            beta = self.backward(observations)  # Обратный проход
            
            # Вычисление градиентов и обновление параметров
            T = len(observations)  # Количество наблюдений в последовательности
            
            # Обновление начальных вероятностей
            self.initial_probabilities = alpha[0] * beta[0] / np.sum(alpha[0] * beta[0])
            
            # Обновление матрицы переходов
            for i in range(self.num_states):
                for j in range(self.num_states):
                    numerator = np.sum(alpha[:-1, i] * self.transition_matrix[i, j] * self.emission_matrix[j, observations[1:]] * beta[1:, j])
                    denominator = np.sum(alpha[:-1, i] * beta[:-1, i])
                    self.transition_matrix[i, j] = numerator / denominator
            
            # Обновление матрицы эмиссий
            for j in range(self.num_states):
                for k in range(self.num_observations):
                    numerator = np.sum((observations == k) * alpha[:, j] * beta[:, j])
                    denominator = np.sum(alpha[:, j] * beta[:, j])
                    self.emission_matrix[j, k] = numerator / denominator


    def predict(self, observations):
        T = len(observations)  # Количество наблюдений в последовательности
        alpha = self.forward(observations)  # Выполняем прямой проход для оценки вероятностей состояний
        
        # Возвращаем вероятности состояний на последнем временном шаге
        return alpha[-1, :]


# Создаем объект HMM с 3 состояниями и 4 возможными наблюдениями
num_states = 3
num_observations = 4
hmm = Baum_Welch(num_states, num_observations)

# Генерируем случайную последовательность наблюдений (например, список целых чисел)
observations = np.random.randint(0, num_observations, size=10)

# Обучаем HMM на сгенерированных наблюдениях
hmm.train(observations)

# Предсказываем последнее состояние для наблюдений
predicted_state_probabilities = hmm.predict(observations)

print("Сгенерированные наблюдения:", observations)
print("Вероятности состояний на последнем временном шаге:", predicted_state_probabilities)


# In[2]:


class Viterbi:
    def __init__(self, hmm_model):
        self.hmm_model = hmm_model  # Передаем объект HMM для использования его параметров

    def find_best_path(self, observations):
        T = len(observations)  # Количество наблюдений в последовательности
        num_states = self.hmm_model.num_states  # Количество скрытых состояний
        
        # Инициализация матрицы для хранения наилучших вероятностей и путей
        viterbi_matrix = np.zeros((T, num_states))
        backpointer_matrix = np.zeros((T, num_states), dtype=int)
        
        # Инициализация первого шага
        viterbi_matrix[0] = self.hmm_model.initial_probabilities * self.hmm_model.emission_matrix[:, observations[0]]
        
        # Прямой проход алгоритма Витерби
        for t in range(1, T):
            for j in range(num_states):
                # Вычисление вероятности наилучшего пути до текущего состояния
                probabilities = viterbi_matrix[t - 1] * self.hmm_model.transition_matrix[:, j]
                max_probability = np.max(probabilities)
                
                # Сохранение наилучшей вероятности и пути
                viterbi_matrix[t, j] = max_probability * self.hmm_model.emission_matrix[j, observations[t]]
                backpointer_matrix[t, j] = np.argmax(probabilities)
        
        # Определение наилучшего пути с помощью обратного прохода
        best_path = [np.argmax(viterbi_matrix[-1])]  # Наилучшее состояние на последнем временном шаге
        
        for t in range(T - 1, 0, -1):
            previous_state = backpointer_matrix[t, best_path[-1]]
            best_path.append(previous_state)
        
        best_path.reverse()  # Инвертируем порядок для получения начала и конца последовательности
        
        return best_path


# In[3]:


# 1. Создание объекта HMM:
num_states = 3
num_observations = 4
hmm = Baum_Welch(num_states, num_observations) # Здесь мы создаем объект HMM с 3 скрытыми состояниями и 4 возможными наблюдениями с помощью класса 
# `Baum_Welch`, который представляет собой модель HMM.

# 2. Генерация случайных наблюдений:
observations = np.random.randint(0, num_observations, size=10)
# Мы генерируем случайную последовательность наблюдений, представленную в виде списка целых чисел. 
# В данном случае, мы генерируем 10 наблюдений из 4 возможных значений.

# 3. Обучение HMM на сгенерированных наблюдениях:
hmm.train(observations)
# Здесь мы обучаем модель HMM на сгенерированных наблюдениях с использованием метода `train` из класса `Baum_Welch`. В результате обучения модель будет настраивать свои параметры (начальные вероятности, матрицу переходов и матрицу эмиссий) так, чтобы лучше соответствовать данным наблюдениям.

# 4. Создание объекта Viterbi:
viterbi_decoder = Viterbi(hmm)
# Мы создаем объект `Viterbi` с использованием обученной модели HMM. Этот объект будет использоваться для поиска наилучшего пути по последовательности наблюдений.

# 5. Нахождение наилучшего пути с использованием алгоритма Витерби:
best_path = viterbi_decoder.find_best_path(observations)
# Здесь мы вызываем метод `find_best_path` из объекта `Viterbi`, передавая ему сгенерированные наблюдения. Этот метод использует алгоритм Витерби для нахождения наилучшей последовательности скрытых состояний, которая наиболее вероятно соответствует наблюдениям.

# 6. Вывод результатов:
print("Сгенерированные наблюдения:", observations)
print("Наилучший путь (последовательность скрытых состояний):", best_path)
# Выводим на экран сгенерированные наблюдения и наилучший путь (последовательность скрытых состояний), найденный с помощью алгоритма Витерби.


# In[4]:


import nltk
# nltk.download('treebank')
# Load the Penn Treebank dataset
corpus = nltk.corpus.treebank.tagged_sents()
# Split the dataset into training and test sets
train_data = corpus[:3000]
test_data = corpus[3000:]
# Train an HMM POS tagger
hmm_tagger = nltk.hmm.HiddenMarkovModelTrainer().train_supervised(train_data)


# In[5]:


# Предложение для тегирования
sentence = "I like tea".split()

# Используйте обученный HMM POS-теггер для предсказания тегов
predicted_tags = hmm_tagger.tag(sentence)

# Выведите предсказанные теги
print(predicted_tags)


# In[ ]:




