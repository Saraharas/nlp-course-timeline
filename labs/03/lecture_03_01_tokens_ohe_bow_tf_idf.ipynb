{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444ebb89-03b2-44ab-851d-4c8659749a4a",
   "metadata": {},
   "source": [
    "# Виды токенизации в nlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b65e01f-114c-49f1-8c0f-5119753e149d",
   "metadata": {},
   "source": [
    "Токенизация в обработке естественного языка (NLP) — это процесс разделения текста на отдельные элементы, называемые токенами. Токены могут быть словами, фразами, символами или даже подстроками, в зависимости от задачи и целей обработки текста. Вот некоторые виды токенизации в NLP:\r\n",
    "\r\n",
    "1. **Токенизация по словам (Word Tokenization)**: Этот вид токенизации разделяет текст на слова. Например, фраза \"Привет, мир!\" будет разделена на токены: [\"Привет\", \",\", \"мир\", \"!\"].\r\n",
    "\r\n",
    "2. **Токенизация по фразам (Phrase Tokenization)**: В некоторых случаях может потребоваться разделить текст на более крупные фрагменты, чем слова, например, на фразы или предложения.\r\n",
    "\r\n",
    "3. **Символьная токенизация (Character Tokenization)**: В этом случае текст разделяется на отдельные символы. Это может быть полезно для некоторых задач, таких как сегментация писем в адреса электронной почты.\r\n",
    "\r\n",
    "4. **Субсловная (Subword) токенизация**: Этот вид токенизации разбивает текст на субслова или морфемы, что полезно для работы с языками, где слова образуются путем комбинации корней и аффиксов. Примером может служить BPE (Byte-Pair Encoding) или токенизация на основе WordPiece.\r\n",
    "\r\n",
    "5. **Токенизация на основе применения правил (Rule-based Tokenization)**: В этом случае используются правила для определения, какие символы служат разделителями токенов. Например, разделение по пробелам или знакам пунктуации.\r\n",
    "\r\n",
    "6. **Токенизация на основе машинного обучения (Machine Learning-based Tokenization)**: В этом случае алгоритмы машинного обучения обучаются на больших объемах текста для автоматического выделения токенов. Примером может служить токенизация на основе рекуррентных нейронных сетей или Transformer-моделей.\r\n",
    "\r\n",
    "7. **Токенизация на уровне символов и подсимвольных элементов (Byte-level and Sub-byte Tokenization)**: Для определенных задач, таких как обработка байтовых данных или кодирование в Unicode, текст может быть разделен на байты, символы или их части (например, биты).\r\n",
    "\r\n",
    "8. **Многоуровневая токенизация (Multi-level Tokenization)**: В некоторых случаях применяются несколько уровней токенизации, например, сначала текст разделяется на слова, а затем каждое слово разделяется на подслова.\r\n",
    "\r\n",
    "Выбор определенного метода токенизации зависит от конкретной задачи, языка и целей анализа текста. Каждый вид токенизации имеет свои преимущества и недостатки, и выбор должен быть сделан с учетом контекста и требований вашего NLP-проекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3721426-7c03-40e5-ac94-fad70ff710dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет,', 'мир!']\n"
     ]
    }
   ],
   "source": [
    "# В Python для токенизации текста по словам вы можете использовать различные библиотеки, такие как NLTK (Natural Language Toolkit) или spaCy, \n",
    "# или же просто разбить строку на слова с помощью методов строк. Вот примеры токенизации по словам с использованием разных методов:\n",
    "\n",
    "# 1. **Разделение строки с помощью метода `split()`:**\n",
    "\n",
    "text = \"Привет, мир!\"\n",
    "tokens = text.split()  # Разделяем текст по пробелам\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c662b7fd-858e-48f2-a195-b3258ab0d8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет', ',', 'мир', '!']\n"
     ]
    }
   ],
   "source": [
    "# 2. **Использование библиотеки NLTK:**\n",
    "# Для этого вам нужно будет установить библиотеку NLTK, если она еще не установлена:\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Привет, мир!\"\n",
    "tokens = word_tokenize(text, language='russian')  # Токенизация по словам с помощью NLTK\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2b88a7-9c2a-409f-8a33-61a5b7bc5ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет', ',', 'мир', '!']\n"
     ]
    }
   ],
   "source": [
    "# 3. **Использование библиотеки spaCy:**\n",
    "# Для использования spaCy также потребуется установить библиотеку и загрузить соответствующую модель:\n",
    "\n",
    "\n",
    "import spacy\n",
    "import spacy.cli\n",
    "\n",
    "# spacy.cli.download(\"ru_core_news_sm\")\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "text = \"Привет, мир!\"\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]  # Токенизация по словам с помощью spaCy\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b7e85dc-14c0-468d-b783-15f459412a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет, мир!', 'Как дела?', 'Это пример токенизации текста на предложения.']\n"
     ]
    }
   ],
   "source": [
    "# Пример токенизации текста не по словам, а по фразам (предложениям) \n",
    "# с использованием библиотеки NLTK в Python:\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Загрузим предобученный токенизатор для разбиения текста на предложения\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Привет, мир! Как дела? Это пример токенизации текста на предложения.\"\n",
    "sentences = sent_tokenize(text, language='russian')  # Токенизация на предложения\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aaa5a24-519e-4bda-b964-1eb9b228cb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['П', 'р', 'и', 'м', 'е', 'р', ' ', 'т', 'о', 'к', 'е', 'н', 'и', 'з', 'а', 'ц', 'и', 'и', ' ', 'п', 'о', ' ', 'с', 'и', 'м', 'в', 'о', 'л', 'а', 'м']\n"
     ]
    }
   ],
   "source": [
    "text = \"Пример токенизации по символам\"\n",
    "tokens = list(text)  # Разделяем текст на символы\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f58b9f-a873-4e14-820d-dad094b4ce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import CharBPETokenizer\n",
    "\n",
    "tokenizer = CharBPETokenizer()\n",
    "\n",
    "tokenizer.train([ \"corpus.txt\", ])\n",
    "\n",
    "tokenizer.save(\"bpe_tokenizer.json\")\n",
    "\n",
    "encoded = tokenizer.encode(\"Чат для NLP\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1900b87-5f6e-49e5-b0e6-e743ba16106e",
   "metadata": {},
   "source": [
    "# Виды нормализации в nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bea14a-906b-495e-b3e7-59df09f5428b",
   "metadata": {},
   "source": [
    "Нормализация в обработке естественного языка (NLP) - это процесс приведения текста к стандартизированному или упрощенному виду, который облегчает анализ и сравнение текстов. Вот некоторые виды нормализации в NLP:\r\n",
    "\r\n",
    "1. **Приведение к нижнему регистру (Lowercasing)**: Приведение всех символов в тексте к нижнему регистру. Например, \"Привет\" и \"привет\" будут считаться одним и тем же словом после нормализации.\r\n",
    "\r\n",
    "2. **Удаление пунктуации**: Удаление знаков препинания, символов и специальных символов из текста. Например, \"Привет!\" станет \"Привет\".\r\n",
    "\r\n",
    "3. **Удаление стоп-слов (Stop Word Removal)**: Удаление часто встречающихся слов, которые не несут смысловой нагрузки (стоп-слова), таких как \"и\", \"в\", \"с\" и т. д.\r\n",
    "\r\n",
    "4. **Стемминг (Stemming)**: Процесс обрезания аффиксов (окончаний) слов для приведения их к их базовой (словарной) форме. Например, слова \"бегу\", \"бежит\" и \"бегут\" могут быть преобразованы к основе \"бег\".\r\n",
    "\r\n",
    "5. **Лемматизация (Lemmatization)**: Процесс приведения слов к их леммам, то есть к их базовым словарным формам. В отличие от стемминга, лемматизация учитывает грамматические правила и обеспечивает более точное приведение слов.\r\n",
    "\r\n",
    "6. **Удаление лишних пробелов**: Удаление дополнительных пробелов между словами и в начале и конце текста.\r\n",
    "\r\n",
    "7. **Замена чисел и дат**: Замена чисел и дат на специальные символы или токены, чтобы сохранить смысл текста, но упростить его структуру.\r\n",
    "\r\n",
    "8. **Коррекция опечаток (Spelling Correction)**: Процесс исправления опечаток и орфографических ошибок в тексте.\r\n",
    "\r\n",
    "9. **Удаление HTML-тегов**: В случае обработки веб-страниц, удаление HTML-тегов для извлечения только текстового содержания.\r\n",
    "\r\n",
    "10. **Обработка сокращений и аббревиатур**: Замена сокращений и аббревиатур на полные формы или наоборот.\r\n",
    "\r\n",
    "11. **Удаление специфичных символов и URL**: Удаление символов, не имеющих отношения к тексту, и URL-адресов.\r\n",
    "\r\n",
    "12. **Нормализация дат и времени**: Приведение дат и времени к стандартному формату.\r\n",
    "\r\n",
    "13. **Удаление символов регулярных выражений**: Удаление символов, соответствующих определенным шаблонам с использованием регулярных выражений.\r\n",
    "\r\n",
    "14. **Перевод символов в транслит**: Преобразование текста на одном языке в транслитерацию другого языка.\r\n",
    "\r\n",
    "Выбор конкретных методов нормализации зависит от задачи и требований проекта. Нормализация текста может быть важной частью предварительной обработки данных в NLP, чтобы улучшить качество анализа и моделирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f86a0192-508d-4e6f-8563-038080b02d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бег\n",
      "сиж\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Выберите язык для стеммера, например, русский\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "word = \"бегу\"\n",
    "stemmed_word = stemmer.stem(word)\n",
    "print(stemmed_word)\n",
    "\n",
    "word = \"сижу\"\n",
    "stemmed_word = stemmer.stem(word)\n",
    "print(stemmed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0a3a40d-decc-49f1-83c4-0f72298613ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бег\n",
      "сидеть\n"
     ]
    }
   ],
   "source": [
    "import pymorphy3\n",
    "\n",
    "# Создайте объект лемматизатора для русского языка\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "word = \"бегу\"\n",
    "lemma = morph.parse(word)[0].normal_form\n",
    "print(lemma)\n",
    "\n",
    "word = \"сижу\"\n",
    "lemma = morph.parse(word)[0].normal_form\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e206e-195f-4c99-a742-f98f184a9c0a",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9455a1de-a29e-485e-aa03-ad2b80114149",
   "metadata": {},
   "source": [
    "Кодирование \"one-hot\" (однозначное кодирование) - это метод представления категориальных данных, таких как слова, символы или другие элементы, в виде бинарных векторов, где каждый элемент вектора соответствует одному элементу категории. В контексте обработки естественного языка (NLP), one-hot encoding используется для представления слов, символов или других элементов текста.\n",
    "\n",
    "Принцип работы one-hot encoding:\n",
    "\n",
    "1. Создается словарь (набор всех уникальных слов или символов), который будет использоваться для кодирования текста.\n",
    "\n",
    "2. Для каждого уникального элемента из словаря создается бинарный вектор фиксированной длины, где все элементы инициализируются нулями.\n",
    "\n",
    "3. Для каждого слова, символа или элемента текста создается соответствующий вектор, и только элемент, соответствующий этому слову или символу в словаре, устанавливается в 1, остальные элементы остаются равными 0.\n",
    "\n",
    "Пример one-hot encoding в NLP:\n",
    "\n",
    "Допустим, у нас есть словарь из следующих слов: [\"я\", \"люблю\", \"NLP\"]. Если у нас есть фраза \"Я люблю NLP\", то one-hot encoding для этой фразы будет выглядеть следующим образом:\n",
    "\n",
    "- \"я\" будет представлено как [1, 0, 0] (первый элемент вектора равен 1, остальные равны 0).\n",
    "- \"люблю\" будет представлено как [0, 1, 0].\n",
    "- \"NLP\" будет представлено как [0, 0, 1].\n",
    "\n",
    "По сути, каждое слово, символ или элемент в тексте кодируется в виде уникального бинарного вектора, что позволяет компьютеру работать с текстовыми данными, представляя их в числовой форме. Однако one-hot encoding имеет ограничения, особенно в случае больших словарей, так как размер векторов может стать очень большим, что может привести к проблемам с вычислительной эффективностью. Тем не менее, это полезный базовый метод для начала работы с текстовыми данными в NLP.\n",
    "\n",
    "\r\n",
    "Преимущества One-Hot Encoding в NLP:\r\n",
    "\r\n",
    "1. **Простота понимания и применения**: One-hot encoding - это простой метод, который легко понимать и реализовывать. Он не требует сложных математических операций и подходит для начинающих в области NLP.\r\n",
    "\r\n",
    "2. **Интерпретируемость**: One-hot векторы явно представляют наличие или отсутствие каждого элемента словаря, что делает их интерпретируемыми.\r\n",
    "\r\n",
    "3. **Инвариантность к порядку**: Порядок слов в тексте не имеет значения при использовании one-hot encoding, потому что каждое слово представлено отдельным бинарным вектором.\r\n",
    "\r\n",
    "Недостатки One-Hot Encoding в NLP:\r\n",
    "\r\n",
    "1. **Размерность векторов**: One-hot encoding приводит к большой размерности векторов, особенно если словарь (набор уникальных слов или символов) большой. Это может привести к высокой вычислительной нагрузке и использованию большого объема памяти.\r\n",
    "\r\n",
    "2. **Отсутствие семантической информации**: Векторы one-hot не содержат семантической информации о словах. Они не учитывают сходство между словами, что может снижать качество некоторых NLP-задач, таких как машинный перевод и анализ тональности.\r\n",
    "\r\n",
    "3. **Разреженность данных**: Векторы one-hot обычно являются разреженными, так как большинство элементов вектора равны нулю. Это увеличивает сложность хранения и обработки данных.\r\n",
    "\r\n",
    "4. **Невозможность обобщения на новые данные**: Если встречается новое слово, которого нет в исходном словаре, его нельзя представить в виде one-hot вектора, что может создать проблемы при обработке неизвестных слов.\r\n",
    "\r\n",
    "5. **Не учитывает смысловые отношения**: One-hot encoding не учитывает семантические отношения между словами, такие как синонимы или антонимы.\r\n",
    "\r\n",
    "Для решения некоторых из этих недостатков могут быть использованы более продвинутые методы кодирования, такие как векторные представления слов (word embeddings), включая Word2Vec, GloVe и FastText, которые способны учить семантические отношения между словами и снижать размерность данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cf53bcb-a8d3-4a82-b1c4-8b839c2815f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "я: [0 0 0 0 1]\n",
      "люблю: [0 0 0 1 0]\n",
      "NLP: [1 0 0 0 0]\n",
      "в: [0 0 1 0 0]\n",
      "Python: [0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Пример списка слов\n",
    "word_list = [\"я\", \"люблю\", \"NLP\", \"в\", \"Python\"]\n",
    "\n",
    "# Создаем объект OneHotEncoder\n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "# Преобразуем список слов в бинарные векторы\n",
    "one_hot_vectors = encoder.fit_transform(word_list)\n",
    "\n",
    "# Выводим результат\n",
    "for word, one_hot_vector in zip(word_list, one_hot_vectors):\n",
    "    print(f\"{word}: {one_hot_vector}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e435790-94ba-4cd0-9228-e628162d5428",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8087d1-7ea7-40de-a5e0-88c2e279f1a9",
   "metadata": {},
   "source": [
    "Модель \"мешок слов\" (Bag of Words, BoW) - это один из базовых методов представления текстовых данных в обработке естественного языка (NLP). Она представляет текст как неупорядоченный набор слов, игнорируя порядок слов и учитывая только их вхождение в текст. Эта концепция имеет несколько ключевых аспектов:\r\n",
    "\r\n",
    "1. **Игнорирование порядка слов**: Модель BoW не учитывает порядок слов в тексте. Она считает, что структура и порядок слов не имеют значения и что каждое слово является независимым от остальных.\r\n",
    "\r\n",
    "2. **Создание словаря**: Прежде чем использовать модель BoW, необходимо создать словарь всех уникальных слов, которые могут встречаться в текстах. Этот словарь будет использоваться для создания векторов признаков.\r\n",
    "\r\n",
    "3. **Подсчет вхождений слов**: Для каждого текста в корпусе подсчитывается, сколько раз каждое слово из словаря встречается в данном тексте.\r\n",
    "\r\n",
    "4. **Представление текстов в виде векторов**: Каждый текст представляется в виде вектора фиксированной длины, где каждый элемент вектора соответствует слову из словаря, а его значение - количеству вхождений этого слова в текст.\r\n",
    "\r\n",
    "Пример:\r\n",
    "Предположим, у нас есть следующие тексты:\r\n",
    "- \"Я люблю программирование.\"\r\n",
    "- \"Программирование интересно.\"\r\n",
    "- \"Программирование - это будущее.\"\r\n",
    "\r\n",
    "Сначала создается словарь всех уникальных слов: [\"Я\", \"люблю\", \"программирование\", \"интересно\", \"это\", \"будущее\"].\r\n",
    "\r\n",
    "Затем каждый текст представляется в виде вектора BoW:\r\n",
    "- \"Я люблю программирование.\" станет [1, 1, 1, 0, 0, 0].\r\n",
    "- \"Программирование интересно.\" станет [0, 0, 1, 1, 0, 0].\r\n",
    "- \"Программирование - это будущее.\" станет [0, 0, 1, 0, 1, 1].\r\n",
    "\r\n",
    "Преимущества и недостатки модели BoW:\r\n",
    "\r\n",
    "Преимущества:\r\n",
    "- Простота и понятность.\r\n",
    "- Эффективность для многих задач, таких как классификация текстов.\r\n",
    "- Позволяет работать с текстами в числовой форме, что необходимо для многих алгоритмов машинного обучения.\r\n",
    "\r\n",
    "Недостатки:\r\n",
    "- Не учитывает семантические отношения между словами.\r\n",
    "- Разреженность данных: векторы BoW часто имеют много нулей, что увеличивает вычислительную сложность.\r\n",
    "- Потеря информации о порядке слов.\r\n",
    "- Не подходит для задач, где важен контекст и семантика, таких как машинный перевод.\r\n",
    "\r\n",
    "Модель BoW является важным концептуальным инструментом в NLP и обычно используется как базовая модель для более сложных методов, таких как TF-IDF и векторные представления слов (Word Embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28aebfef-bc80-4891-ad65-f3a29f4b06ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица Bag of Words:\n",
      "[[0 0 0 0 0 0 0 1 0 1 0 1]\n",
      " [0 0 1 0 0 0 1 0 1 0 0 1]\n",
      " [1 1 0 1 1 1 0 0 0 0 1 0]]\n",
      "\n",
      "Список слов:\n",
      "['быть' 'длинными' 'еще' 'или' 'короткими' 'могут' 'один' 'пример' 'текст'\n",
      " 'текста' 'тексты' 'это']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Пример корпуса текстов\n",
    "corpus = [\n",
    "    \"Это пример текста 1.\",\n",
    "    \"Это еще один текст.\",\n",
    "    \"Тексты могут быть короткими или длинными.\"\n",
    "]\n",
    "\n",
    "# Создаем объект CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Преобразуем корпус в матрицу Bag of Words\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Получаем список слов (терминов)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Выводим матрицу Bag of Words\n",
    "print(\"Матрица Bag of Words:\")\n",
    "print(X.toarray())\n",
    "\n",
    "# Выводим список слов\n",
    "print(\"\\nСписок слов:\")\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f2d88-783b-4e62-a3a4-81300b85d22e",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93c833-ed5c-406e-8c0a-130004bc007e",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) - это статистическая мера, используемая в обработке естественного языка (NLP) для оценки важности слова в документе относительно корпуса документов. TF-IDF представляет собой комбинацию двух компонентов:\n",
    "\n",
    "1. **TF (Term Frequency)** - Частота термина: это мера, которая оценивает, насколько часто слово встречается в данном документе. Она измеряет, насколько слово важно для конкретного документа.\n",
    "\n",
    "2. **IDF (Inverse Document Frequency)** - Обратная частота документа: это мера, которая оценивает, насколько слово уникально для всего корпуса документов. Она измеряет, насколько слово важно в контексте всего корпуса.\n",
    "\n",
    "Комбинируя TF и IDF, мы можем вычислить TF-IDF для каждого слова в документе. Формула для вычисления TF-IDF для слова `w` в документе `d` обычно выглядит следующим образом:\n",
    "\n",
    "```\n",
    "TF-IDF(w, d) = TF(w, d) * IDF(w)\n",
    "```\n",
    "\n",
    "Где:\n",
    "- `TF(w, d)` - Частота термина (Term Frequency) слова `w` в документе `d`. Это может быть просто количество вхождений слова в документе или нормализованное значение, например, частота встречаемости слова в документе относительно общей длины документа.\n",
    "- `IDF(w)` - Обратная частота документа (Inverse Document Frequency) слова `w`. Это значение вычисляется как логарифм обратного отношения общего числа документов в корпусе к числу документов, содержащих слово `w`. Формула может быть слегка варьирована для учета различных вариантов.\n",
    "\n",
    "Преимущества TF-IDF в NLP:\n",
    "- Помогает выделить важные слова в документах.\n",
    "- Учитывает частоту и уникальность слова.\n",
    "- Хорошо работает для выявления ключевых слов и тематического моделирования.\n",
    "- Используется для ранжирования документов в поисковых системах.\n",
    "\n",
    "Недостатки TF-IDF в NLP:\n",
    "- Не учитывает семантику слов.\n",
    "- Не учитывает порядок слов в тексте.\n",
    "- Не обрабатывает синонимы и многозначные слова.\n",
    "\n",
    "TF-IDF - это полезная мера для анализа и обработки текстовых данных, и она часто используется в задачах информационного поиска, классификации текстов и кластеризации документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60f33c61-8313-4fd0-ad38-1f0bd3216953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Матрица:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.62276601 0.         0.62276601 0.         0.4736296 ]\n",
      " [0.         0.         0.52863461 0.         0.         0.\n",
      "  0.52863461 0.         0.52863461 0.         0.         0.40204024]\n",
      " [0.40824829 0.40824829 0.         0.40824829 0.40824829 0.40824829\n",
      "  0.         0.         0.         0.         0.40824829 0.        ]]\n",
      "\n",
      "Список слов:\n",
      "['быть' 'длинными' 'еще' 'или' 'короткими' 'могут' 'один' 'пример' 'текст'\n",
      " 'текста' 'тексты' 'это']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Пример корпуса текстов\n",
    "corpus = [\n",
    "    \"Это пример текста 1.\",\n",
    "    \"Это еще один текст.\",\n",
    "    \"Тексты могут быть короткими или длинными.\"\n",
    "]\n",
    "\n",
    "# Создаем объект TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Преобразуем корпус в TF-IDF матрицу\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Получаем список слов (терминов)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Выводим TF-IDF матрицу\n",
    "print(\"TF-IDF Матрица:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Выводим список слов\n",
    "print(\"\\nСписок слов:\")\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490fc2a-0b58-454b-8550-8bce13d62d1d",
   "metadata": {},
   "source": [
    "BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e6ce4-f930-4b1c-b708-e7c525b1136a",
   "metadata": {},
   "source": [
    "BM25 (Best Matching 25) - это вероятностная модель ранжирования для информационного поиска и анализа текста. BM25 является улучшенной версией более ранней модели BM25, которая была представлена в 1994 году. BM25 используется для оценки релевантности документов по отношению к запросу пользователя в системах поиска, включая поисковые движки, и для анализа текстовых данных.\r\n",
    "\r\n",
    "Основные концепции BM25:\r\n",
    "\r\n",
    "1. **Веса терминов**: BM25 учитывает веса терминов (слов) в запросе и документе. Веса терминов определяют, насколько важен каждый термин для релевантности документа запросу.\r\n",
    "\r\n",
    "2. **Длина документа**: BM25 учитывает длину документа. Это означает, что более длинные документы могут иметь более низкие оценки релевантности, если они содержат много терминов, которые не совпадают с запросом.\r\n",
    "\r\n",
    "3. **IDF (Inverse Document Frequency)**: BM25 использует обратную документную частоту (IDF) для учета уникальности терминов в корпусе документов. Термины, которые встречаются редко в корпусе, имеют более высокие веса.\r\n",
    "\r\n",
    "4. **Параметры ранжирования**: BM25 имеет несколько параметров ранжирования, таких как параметр \"k\" и параметр \"b\", которые можно настроить под конкретные задачи и типы данных.\r\n",
    "\r\n",
    "5. **Вероятностное ранжирование**: BM25 представляет собой вероятностную модель ранжирования, которая оценивает вероятность, что документ будет релевантным для запроса.\r\n",
    "\r\n",
    "Преимущества BM25 в NLP:\r\n",
    "\r\n",
    "- BM25 является одним из наиболее эффективных алгоритмов ранжирования для поисковых систем.\r\n",
    "- Он хорошо работает с короткими и длинными текстами.\r\n",
    "- Учитывает уникальность и веса терминов, что делает его более точным по сравнению с некоторыми более простыми моделями.\r\n",
    "\r\n",
    "Недостатки BM25 в NLP:\r\n",
    "\r\n",
    "- Требует настройки параметров для оптимальной производительности.\r\n",
    "- Может быть вычислительно затратным при обработке больших корпусов документов.\r\n",
    "- Не учитывает семантические отношения между словами.\r\n",
    "\r\n",
    "BM25 широко используется в поисковых системах и информационном поиске для ранжирования и выдачи наиболее релевантных документов для пользовательских запросов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4dd8af1-2d21-47e5-8d4d-45373412e5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Документ 1: BM25 Score = 0.598\n",
      "Документ 2: BM25 Score = 0.052\n",
      "Документ 3: BM25 Score = 0.043\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Пример корпуса текстов\n",
    "corpus = [\n",
    "    \"Это пример текст 1.\",\n",
    "    \"Это еще один текст\",\n",
    "    \"текст могут быть короткими или длинными.\"\n",
    "]\n",
    "\n",
    "# Разделите тексты на слова (токенизация)\n",
    "tokenized_corpus = [text.split() for text in corpus]\n",
    "\n",
    "# Создайте модель BM25\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Запрос\n",
    "query = \"пример текст\"\n",
    "\n",
    "# Токенизируйте запрос\n",
    "query_tokens = query.split()\n",
    "\n",
    "# Вычислите BM25 для запроса и корпуса\n",
    "scores = bm25.get_scores(query_tokens)\n",
    "\n",
    "# Выведите результаты (релевантность документов по запросу)\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"Документ {i + 1}: BM25 Score = {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22b5ba-39e0-4f6b-b8b8-87626b3b1d68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
