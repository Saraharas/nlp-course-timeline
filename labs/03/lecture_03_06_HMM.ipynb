{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8e206e-195f-4c99-a742-f98f184a9c0a",
   "metadata": {},
   "source": [
    "# HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0f20b-5884-40b4-8792-7787e237810e",
   "metadata": {},
   "source": [
    "Скрытые марковские модели (СММ) — это статистическая модель, используемая для моделирования последовательностей данных, которые могут быть представлены в виде последовательности скрытых состояний, связанных с наблюдаемыми данными. СММ являются одним из ключевых инструментов в областях машинного обучения и обработки сигналов, и они нашли широкое применение в ряде областей, таких как распознавание речи, распознавание образов, биоинформатика, финансовая аналитика и другие.\n",
    "\n",
    "Основные компоненты СММ включают в себя:\n",
    "\n",
    "1. Скрытые состояния (Hidden States): Эти состояния не наблюдаются напрямую, но они описывают скрытую структуру модели и изменяются во времени согласно некоторым вероятностным правилам.\n",
    "\n",
    "2. Наблюдаемые данные (Observations): Эти данные наблюдаются и используются для оценки скрытых состояний. Наблюдения связаны с текущим скрытым состоянием через вероятностные законы.\n",
    "\n",
    "3. Вероятностные переходы (Transition Probabilities): СММ определяют вероятности переходов между скрытыми состояниями во времени. Эти вероятности описывают, как скрытое состояние эволюционирует во времени.\n",
    "\n",
    "4. Вероятности наблюдений (Emission Probabilities): Эти вероятности определяют вероятность наблюдать конкретное значение при данном скрытом состоянии.\n",
    "\n",
    "Скрытые марковские модели часто используются для задач, связанных с последовательностями, такими как распознавание текста, прогнозирование временных рядов, анализ последовательностей ДНК и белков, а также в ряде других приложений, где моделирование последовательностей данных является важным элементом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee663b-837c-4bcf-a744-6c459d28b147",
   "metadata": {},
   "source": [
    "Алгоритм Баума-Велша (Baum-Welch) - это алгоритм обучения скрытой марковской модели (HMM) при заданной последовательности наблюдаемых данных.\n",
    "\n",
    "Алгоритм Baum-Welch является одним из ключевых методов для обучения параметров скрытой марковской модели (HMM) при наличии только наблюдаемых данных. Он является частью более общего алгоритма, известного как алгоритм Expectation-Maximization (EM), который используется для оценки параметров статистических моделей в случае, когда некоторые переменные (в данном случае, скрытые состояния) недоступны для наблюдения.\r\n",
    "\r\n",
    "Суть алгоритма Baum-Welch заключается в том, чтобы находить наилучшие оценки параметров HMM, максимизируя правдоподобие наблюдаемых данных при условии модели. Он выполняет это обновлением параметров итеративно в два шага:\r\n",
    "\r\n",
    "1. **Expectation Step (E-шаг)**: На этом шаге алгоритм вычисляет ожидания скрытых состояний (посредством прямого и обратного проходов) для заданных наблюдений и текущих параметров модели.\r\n",
    "\r\n",
    "2. **Maximization Step (M-шаг)**: На этом шаге алгоритм обновляет параметры модели (начальные вероятности, матрицу переходов и матрицу эмиссий) так, чтобы максимизировать правдоподобие наблюдаемых данных, учитывая ожидания скрытых состояний, полученные на E-шаге.\r\n",
    "\r\n",
    "Эти два шага выполняются итеративно до сходимости (или до достижения заданного числа итераций). В конечном итоге, Baum-Welch адаптирует параметры HMM к данным, что позволяет модели более точно описывать наблюдаемые последовательности.\r\n",
    "\r\n",
    "Сценарии применения алгоритма Baum-Welch включают обучение HMM для ряда задач, таких как распознавание речи, обработка естественного языка, анализ временных рядов и другие, где присутствует скрытая структура и наблюдаем\n",
    "\n",
    "Алгоритм Витерби является важным компонентом работы со скрытыми марковскими моделями (HMM) и имеет несколько важных применений:\r\n",
    "\r\n",
    "1. **Наилучший путь (Best Path) в HMM**: Основное применение алгоритма Витерби - это нахождение наиболее вероятной последовательности скрытых состояний в HMM для заданной последовательности наблюдаемых данных. Это полезно во многих задачах, таких как распознавание речи, где нужно определить, какие слова или фонемы наиболее вероятно соответствуют записи речи.\r\n",
    "\r\n",
    "2. **Выравнивание последовательностей**: Алгоритм Витерби может использоваться для выравнивания двух последовательностей, одна из которых является наблюдаемой, а другая - скрытой. Это применение находит свое применение в биоинформатике, где можно определять гомологичные участки геномов, выравнивая последовательности нуклеотидов.\r\n",
    "\r\n",
    "3. **Декодирование кодовых последовательностей**: Алгоритм Витерби используется в декодировании кодовых последовательностей, таких как коды Хэмминга, коды Хаффмана и другие. Он помогает определить наиболее вероятное исходное сообщение на основе полученных данных.\r\n",
    "\r\n",
    "4. **Сжатие данных**: В контексте сжатия данных, алгоритм Витерби используется для декодирования сжатых последовательностей. Например, в беспроводных связях, алгоритм Витерби может быть применен для восстановления данных, полученных с помощью кодирования с помощью сверточных кодов.\r\n",
    "\r\n",
    "5. **Разрешение амбигуитетов**: В различных областях, где есть амбигуитеты или неопределенности, алгоритм Витерби может помочь выбрать наилучший вариант. Например, в машинном переводе, где несколько переводов могут быть возможными, алгоритм Витерби может помочь выбрать наиболее `transition_matrix` и `emission_matrix` - это две важные матрицы, используемые в модели скрытой марковской модели (HMM) для моделирования последовательности наблюдений и скрытых состояний.\r\n",
    "\r\n",
    "1. **Матрица переходов (Transition Matrix):**\r\n",
    "   - `transition_matrix` представляет собой матрицу размера `num_states x num_states`, где `num_states` - это количество скрытых состояний в HMM.\r\n",
    "   - Каждый элемент `transition_matrix[i][j]` представляет вероятность перехода из скрытого состояния `i` в скрытое состояние `j`. То есть, она определяет вероятности переходов между скрытыми состояниями на каждом временном шаге.\r\n",
    "   - Например, `transition_matrix[0][1]` представляет вероятность перехода из состояния 0 в состояние 1.\r\n",
    "\r\n",
    "2. **Матрица эмиссий (Emission Matrix):**\r\n",
    "   - `emission_matrix` также является матрицей, размер которой `num_states x num_observations`. \r\n",
    "   - Каждый элемент `emission_matrix[i][k]` представляет вероятность наблюдения `k` при нахождении системы в скрытом состоянии `i`. Это определяет вероятность того, какие наблюдения связаны с каждым из скрытых состояний.\r\n",
    "   - Например, `emission_matrix[1][2]` представляет вероятность наблюдения 2 (или другой символ, в зависимости от вашей задачи) при нахождении системы в состоянии 1.\r\n",
    "\r\n",
    "Вместе `transition_matrix` и `emission_matrix` определяют модель HMM, которая используется для оценки вероятностей скрытых состояний и наблюдений во временных последовательностях. Эти матрицы обучаются с использованием алгоритма Баума-Велша на основе наблюдаемых данных.ерная лингвистика и мн\n",
    "\n",
    "В контексте скрытой марковской модели (HMM) \"скрытые состояния\" и \"наблюдения\" - это ключевые концепции, которые используются для \n",
    "моделирования и анализа последовательностей данных. Давайте объясним эти концепции на примере.\n",
    "\n",
    "1. **Скрытые состояния (Hidden States):**  \n",
    "   Скрытые состояния - это некие абстрактные состояния, которые не наблюдаются непосредственно, но влияют на наблюдаемые данные. \n",
    "   Эти состояния могут быть интерпретированы как внутренние состояния системы, которые меняются со временем. В контексте HMM, \n",
    "   скрытые состояния формируют марковскую цепь, то есть переходы между состояниями зависят только от текущего состояния и не зависят \n",
    "   от предыдущих состояний. В вашем примере `num_states = 3`, это означает, что у вас есть 3 различных скрытых состояния, которые могут \n",
    "   быть обозначены, например, как \"Состояние 1\", \"Состояние 2\" и \"Состояние 3\".\n",
    "\n",
    "2. **Наблюдения (Observations):**  \n",
    "   Наблюдения - это данные, которые мы непосредственно наблюдаем или измеряем. Они связаны с скрытыми состояниями, и каждое скрытое \n",
    "   состояние может производить разные наблюдения с разными вероятностями. В контексте HMM, наблюдения могут быть категоризированы, \n",
    "   и `num_observations = 4` означает, что у нас есть 4 возможных категории наблюдений. Например, если мы рассматриваем модель для \n",
    "   текстовой последовательности, наблюдения могут представлять собой слова, и `num_observations` будет равно количеству различных \n",
    "   слов в нашем словаре.\n",
    "\n",
    "Давайте представим простой пример. Пусть у нас есть HMM для моделирования погоды:\n",
    "\n",
    "- Скрытые состояния (HMM состояния): \"Солнечно\", \"Пасмурно\", \"Дождь\".\n",
    "- Наблюдения: \"Зонт\", \"Очки\", \"Зонт\", \"Зонт\", \"Очки\".\n",
    "\n",
    "В этом примере, скрытые состояния представляют разные погодные условия, которые мы не наблюдаем напрямую, а наблюдения - это вещи, \n",
    "которые мы можем видеть, например, носить зонт или солнечные очки. Задача HMM - определить, какие погодные условия (скрытые состояния) \n",
    "наиболее вероятно привели к данным наблюдениям (вещам, которые мы видим).огие другие.ые данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a47fa1e-ac68-42ca-809b-257d57bce0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сгенерированные наблюдения: [3 3 3 2 3 3 2 1 0 3]\n",
      "Вероятности состояний на последнем временном шаге: [8.62188282e-111 3.20830629e-010 4.81708683e-004]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Baum_Welch:\n",
    "    def __init__(self, num_states, num_observations):\n",
    "        # Инициализация класса Baum_Welch с заданным количеством состояний и наблюдений.\n",
    "        self.num_states = num_states  # Хранение количества скрытых состояний.\n",
    "        self.num_observations = num_observations  # Хранение количества возможных наблюдений.\n",
    "        \n",
    "        # Инициализация начальных вероятностей состояний случайными значениями и их нормализация.\n",
    "        self.initial_probabilities = np.random.rand(num_states)\n",
    "        self.initial_probabilities /= np.sum(self.initial_probabilities)\n",
    "        \n",
    "        # Инициализация матрицы переходов между состояниями случайными значениями и их нормализация по строкам.\n",
    "        self.transition_matrix = np.random.rand(num_states, num_states)\n",
    "        self.transition_matrix /= np.sum(self.transition_matrix, axis=1, keepdims=True)\n",
    "        \n",
    "        # Инициализация матрицы эмиссий случайными значениями и их нормализация по строкам.\n",
    "        self.emission_matrix = np.random.rand(num_states, num_observations)\n",
    "        self.emission_matrix /= np.sum(self.emission_matrix, axis=1, keepdims=True)\n",
    "\n",
    "    \n",
    "    def forward(self, observations):\n",
    "        # Прямой проход (forward pass) с использованием алгоритма Витерби\n",
    "        T = len(observations)  # Количество наблюдений в последовательности\n",
    "        alpha = np.zeros((T, self.num_states))  # Матрица альфа для хранения промежуточных значений\n",
    "        \n",
    "        # Инициализация первого шага алгоритма\n",
    "        alpha[0] = self.initial_probabilities * self.emission_matrix[:, observations[0]]\n",
    "        \n",
    "        for t in range(1, T):\n",
    "            for j in range(self.num_states):\n",
    "                # Рекурсивное вычисление альфа для каждого состояния на текущем временном шаге\n",
    "                alpha[t, j] = np.sum(alpha[t - 1] * self.transition_matrix[:, j]) * self.emission_matrix[j, observations[t]]\n",
    "        \n",
    "        return alpha  # Возвращение матрицы альфа, содержащей прямые проходы для всех временных шагов\n",
    "\n",
    "    \n",
    "    def backward(self, observations):\n",
    "        # Обратный проход (backward pass)\n",
    "        T = len(observations)  # Количество наблюдений в последовательности\n",
    "        beta = np.zeros((T, self.num_states))  # Матрица бета для хранения промежуточных значений\n",
    "        beta[-1] = 1.0  # Инициализация последнего шага обратного прохода\n",
    "        \n",
    "        # Обратный проход начиная с предпоследнего временного шага\n",
    "        for t in range(T - 2, -1, -1):\n",
    "            for i in range(self.num_states):\n",
    "                # Рекурсивное вычисление бета для каждого состояния на текущем временном шаге\n",
    "                beta[t, i] = np.sum(self.transition_matrix[i, :] * self.emission_matrix[:, observations[t + 1]] * beta[t + 1, :])\n",
    "        \n",
    "        return beta  # Возвращение матрицы бета, содержащей обратные проходы для всех временных шагов\n",
    "\n",
    "    \n",
    "    def train(self, observations, num_iterations=100):\n",
    "        # Обучение параметров HMM с использованием алгоритма Баума-Велша\n",
    "        for iteration in range(num_iterations):\n",
    "            alpha = self.forward(observations)  # Прямой проход\n",
    "            beta = self.backward(observations)  # Обратный проход\n",
    "            \n",
    "            # Вычисление градиентов и обновление параметров\n",
    "            T = len(observations)  # Количество наблюдений в последовательности\n",
    "            \n",
    "            # Обновление начальных вероятностей\n",
    "            self.initial_probabilities = alpha[0] * beta[0] / np.sum(alpha[0] * beta[0])\n",
    "            \n",
    "            # Обновление матрицы переходов\n",
    "            for i in range(self.num_states):\n",
    "                for j in range(self.num_states):\n",
    "                    numerator = np.sum(alpha[:-1, i] * self.transition_matrix[i, j] * self.emission_matrix[j, observations[1:]] * beta[1:, j])\n",
    "                    denominator = np.sum(alpha[:-1, i] * beta[:-1, i])\n",
    "                    self.transition_matrix[i, j] = numerator / denominator\n",
    "            \n",
    "            # Обновление матрицы эмиссий\n",
    "            for j in range(self.num_states):\n",
    "                for k in range(self.num_observations):\n",
    "                    numerator = np.sum((observations == k) * alpha[:, j] * beta[:, j])\n",
    "                    denominator = np.sum(alpha[:, j] * beta[:, j])\n",
    "                    self.emission_matrix[j, k] = numerator / denominator\n",
    "\n",
    "\n",
    "    def predict(self, observations):\n",
    "        T = len(observations)  # Количество наблюдений в последовательности\n",
    "        alpha = self.forward(observations)  # Выполняем прямой проход для оценки вероятностей состояний\n",
    "        \n",
    "        # Возвращаем вероятности состояний на последнем временном шаге\n",
    "        return alpha[-1, :]\n",
    "\n",
    "\n",
    "# Создаем объект HMM с 3 состояниями и 4 возможными наблюдениями\n",
    "num_states = 3\n",
    "num_observations = 4\n",
    "hmm = Baum_Welch(num_states, num_observations)\n",
    "\n",
    "# Генерируем случайную последовательность наблюдений (например, список целых чисел)\n",
    "observations = np.random.randint(0, num_observations, size=10)\n",
    "\n",
    "# Обучаем HMM на сгенерированных наблюдениях\n",
    "hmm.train(observations)\n",
    "\n",
    "# Предсказываем последнее состояние для наблюдений\n",
    "predicted_state_probabilities = hmm.predict(observations)\n",
    "\n",
    "print(\"Сгенерированные наблюдения:\", observations)\n",
    "print(\"Вероятности состояний на последнем временном шаге:\", predicted_state_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e830fa8-9d84-46de-8f9e-94ad3d47c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Viterbi:\n",
    "    def __init__(self, hmm_model):\n",
    "        self.hmm_model = hmm_model  # Передаем объект HMM для использования его параметров\n",
    "\n",
    "    def find_best_path(self, observations):\n",
    "        T = len(observations)  # Количество наблюдений в последовательности\n",
    "        num_states = self.hmm_model.num_states  # Количество скрытых состояний\n",
    "        \n",
    "        # Инициализация матрицы для хранения наилучших вероятностей и путей\n",
    "        viterbi_matrix = np.zeros((T, num_states))\n",
    "        backpointer_matrix = np.zeros((T, num_states), dtype=int)\n",
    "        \n",
    "        # Инициализация первого шага\n",
    "        viterbi_matrix[0] = self.hmm_model.initial_probabilities * self.hmm_model.emission_matrix[:, observations[0]]\n",
    "        \n",
    "        # Прямой проход алгоритма Витерби\n",
    "        for t in range(1, T):\n",
    "            for j in range(num_states):\n",
    "                # Вычисление вероятности наилучшего пути до текущего состояния\n",
    "                probabilities = viterbi_matrix[t - 1] * self.hmm_model.transition_matrix[:, j]\n",
    "                max_probability = np.max(probabilities)\n",
    "                \n",
    "                # Сохранение наилучшей вероятности и пути\n",
    "                viterbi_matrix[t, j] = max_probability * self.hmm_model.emission_matrix[j, observations[t]]\n",
    "                backpointer_matrix[t, j] = np.argmax(probabilities)\n",
    "        \n",
    "        # Определение наилучшего пути с помощью обратного прохода\n",
    "        best_path = [np.argmax(viterbi_matrix[-1])]  # Наилучшее состояние на последнем временном шаге\n",
    "        \n",
    "        for t in range(T - 1, 0, -1):\n",
    "            previous_state = backpointer_matrix[t, best_path[-1]]\n",
    "            best_path.append(previous_state)\n",
    "        \n",
    "        best_path.reverse()  # Инвертируем порядок для получения начала и конца последовательности\n",
    "        \n",
    "        return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f9ed7c-f7ad-42e8-b0b2-1b6328ce6a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сгенерированные наблюдения: [0 1 1 3 2 1 2 3 2 1]\n",
      "Наилучший путь (последовательность скрытых состояний): [1, 2, 2, 0, 1, 0, 1, 0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# 1. Создание объекта HMM:\n",
    "num_states = 3\n",
    "num_observations = 4\n",
    "hmm = Baum_Welch(num_states, num_observations) # Здесь мы создаем объект HMM с 3 скрытыми состояниями и 4 возможными наблюдениями с помощью класса \n",
    "# `Baum_Welch`, который представляет собой модель HMM.\n",
    "\n",
    "# 2. Генерация случайных наблюдений:\n",
    "observations = np.random.randint(0, num_observations, size=10)\n",
    "# Мы генерируем случайную последовательность наблюдений, представленную в виде списка целых чисел. \n",
    "# В данном случае, мы генерируем 10 наблюдений из 4 возможных значений.\n",
    "\n",
    "# 3. Обучение HMM на сгенерированных наблюдениях:\n",
    "hmm.train(observations)\n",
    "# Здесь мы обучаем модель HMM на сгенерированных наблюдениях с использованием метода `train` из класса `Baum_Welch`. В результате обучения модель будет настраивать свои параметры (начальные вероятности, матрицу переходов и матрицу эмиссий) так, чтобы лучше соответствовать данным наблюдениям.\n",
    "\n",
    "# 4. Создание объекта Viterbi:\n",
    "viterbi_decoder = Viterbi(hmm)\n",
    "# Мы создаем объект `Viterbi` с использованием обученной модели HMM. Этот объект будет использоваться для поиска наилучшего пути по последовательности наблюдений.\n",
    "\n",
    "# 5. Нахождение наилучшего пути с использованием алгоритма Витерби:\n",
    "best_path = viterbi_decoder.find_best_path(observations)\n",
    "# Здесь мы вызываем метод `find_best_path` из объекта `Viterbi`, передавая ему сгенерированные наблюдения. Этот метод использует алгоритм Витерби для нахождения наилучшей последовательности скрытых состояний, которая наиболее вероятно соответствует наблюдениям.\n",
    "\n",
    "# 6. Вывод результатов:\n",
    "print(\"Сгенерированные наблюдения:\", observations)\n",
    "print(\"Наилучший путь (последовательность скрытых состояний):\", best_path)\n",
    "# Выводим на экран сгенерированные наблюдения и наилучший путь (последовательность скрытых состояний), найденный с помощью алгоритма Витерби."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2907e404-2910-49ff-b763-a5595cc4b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('treebank')\n",
    "# Load the Penn Treebank dataset\n",
    "corpus = nltk.corpus.treebank.tagged_sents()\n",
    "# Split the dataset into training and test sets\n",
    "train_data = corpus[:3000]\n",
    "test_data = corpus[3000:]\n",
    "# Train an HMM POS tagger\n",
    "hmm_tagger = nltk.hmm.HiddenMarkovModelTrainer().train_supervised(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "779a8e4b-f766-4e57-ade5-8f894ba0ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avglinsky\\PycharmProjects\\nlp-course-timeline\\venv\\Lib\\site-packages\\nltk\\tag\\hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
      "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
      "C:\\Users\\avglinsky\\PycharmProjects\\nlp-course-timeline\\venv\\Lib\\site-packages\\nltk\\tag\\hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "C:\\Users\\avglinsky\\PycharmProjects\\nlp-course-timeline\\venv\\Lib\\site-packages\\nltk\\tag\\hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
      "  P[i] = self._priors.logprob(si)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('like', 'IN'), ('tea', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Предложение для тегирования\n",
    "sentence = \"I like tea\".split()\n",
    "\n",
    "# Используйте обученный HMM POS-теггер для предсказания тегов\n",
    "predicted_tags = hmm_tagger.tag(sentence)\n",
    "\n",
    "# Выведите предсказанные теги\n",
    "print(predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d0a2e-12fa-4ea5-987d-9f40816fd272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
