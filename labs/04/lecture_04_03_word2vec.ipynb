{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 589.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('brown', 0.20187540173598992), ('quick', 0.11300073197633051), ('lazy', 0.10937511690673239), ('over', 0.10454915646347869), ('the', 0.08018912724814303)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Word2VecSkipGram:\n",
    "    def __init__(self, window_size=2, n=100, epochs=200, learning_rate=0.01):\n",
    "        self.window_size = window_size\n",
    "        self.n = n\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def generate_training_data(self, corpus):\n",
    "        # Инициализация словаря для подсчета частоты слов\n",
    "        word_count = defaultdict(int)\n",
    "    \n",
    "        # Подсчитываем частоту каждого слова в корпусе\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                word_count[word] += 1\n",
    "    \n",
    "        # Создаем отображение слов в индексы и обратно\n",
    "        self.word2index = {word: i for i, word in enumerate(word_count.keys())}\n",
    "        self.index2word = {i: word for word, i in self.word2index.items()}\n",
    "        \n",
    "        # Определяем размер словаря (количество уникальных слов)\n",
    "        self.vocab_size = len(self.word2index)\n",
    "    \n",
    "        # Создаем список обучающих пар (target_word, context_word)\n",
    "        training_data = []\n",
    "        for sentence in corpus:\n",
    "            for i, target_word in enumerate(sentence):\n",
    "                for j in range(i - self.window_size, i + self.window_size + 1):\n",
    "                    if j != i and 0 <= j < len(sentence):\n",
    "                        context_word = sentence[j]\n",
    "                        training_data.append((target_word, context_word))\n",
    "    \n",
    "        return training_data\n",
    "\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.W1 = np.random.uniform(-1, 1, (self.vocab_size, self.n))\n",
    "        self.W2 = np.random.uniform(-1, 1, (self.n, self.vocab_size))\n",
    "\n",
    "    def forward(self, target_word):\n",
    "        # Получаем индекс целевого слова в словаре\n",
    "        target_idx = self.word2index[target_word]\n",
    "        \n",
    "        # Инициализируем входной слой (input layer) нулевым вектором\n",
    "        self.input_layer = np.zeros(self.vocab_size)\n",
    "        \n",
    "        # Устанавливаем значение индекса целевого слова в 1, чтобы представить его как one-hot вектор\n",
    "        self.input_layer[target_idx] = 1\n",
    "        \n",
    "        # Производим вычисление скрытого слоя (hidden layer) путем умножения входного слоя на матрицу W1\n",
    "        self.hidden_layer = np.dot(self.input_layer, self.W1)\n",
    "        \n",
    "        # Производим вычисление выходного слоя (output layer) путем умножения скрытого слоя на матрицу W2\n",
    "        self.output_layer = np.dot(self.hidden_layer, self.W2)\n",
    "        \n",
    "        # Применяем softmax к выходному слою для получения вероятностей контекстных слов\n",
    "        self.output_probs = self.softmax(self.output_layer)\n",
    "        \n",
    "        # Возвращаем выходные вероятности контекстных слов\n",
    "        return self.output_probs\n",
    "\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    def backprop(self, target_word, context_word):\n",
    "        # Получаем индексы целевого и контекстного слов в словаре\n",
    "        target_idx = self.word2index[target_word]\n",
    "        context_idx = self.word2index[context_word]\n",
    "    \n",
    "        # Вычисляем разницу между предсказанными вероятностями и фактическими вероятностями\n",
    "        delta_output = self.output_probs\n",
    "        delta_output[context_idx] -= 1\n",
    "    \n",
    "        # Вычисляем ошибку на скрытом слое, умножая ошибку на выходном слое на матрицу W2\n",
    "        delta_hidden = np.dot(self.W2, delta_output)\n",
    "    \n",
    "        # Обновляем матрицу W1 с учетом ошибки на скрытом слое и входного слоя\n",
    "        self.W1 -= self.learning_rate * np.outer(self.input_layer, delta_hidden)\n",
    "    \n",
    "        # Обновляем матрицу W2 с учетом ошибки на выходном слое и скрытого слоя\n",
    "        self.W2 -= self.learning_rate * np.outer(self.hidden_layer, delta_output)\n",
    "\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # Генерируем обучающие данные из предоставленного корпуса\n",
    "        training_data = self.generate_training_data(corpus)\n",
    "        \n",
    "        # Инициализируем веса модели\n",
    "        self.initialize_weights()\n",
    "    \n",
    "        # Начинаем обучение на заданном количестве эпох (self.epochs)\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            # Проходим по всем обучающим парам (target_word, context_word) в обучающих данных\n",
    "            for target_word, context_word in training_data:\n",
    "                # Выполняем прямой проход (forward pass) для текущего целевого слова\n",
    "                self.forward(target_word)\n",
    "                \n",
    "                # Выполняем обратное распространение (backpropagation) и обновляем веса модели\n",
    "                self.backprop(target_word, context_word)\n",
    "\n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        if word in self.word2index:\n",
    "            target_idx = self.word2index[word]\n",
    "            return self.W1[target_idx]\n",
    "\n",
    "    def most_similar(self, word, top_n=10):\n",
    "        if word in self.word2index:\n",
    "            target_vector = self.get_word_vector(word)\n",
    "            word_sim = {}\n",
    "\n",
    "            for i in range(self.vocab_size):\n",
    "                if i != self.word2index[word]:\n",
    "                    context_word = self.index2word[i]\n",
    "                    context_vector = self.get_word_vector(context_word)\n",
    "                    cosine_similarity = np.dot(target_vector, context_vector) / (np.linalg.norm(target_vector) * np.linalg.norm(context_vector))\n",
    "                    word_sim[context_word] = cosine_similarity\n",
    "\n",
    "            return sorted(word_sim.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "# Пример использования\n",
    "corpus = [['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']]\n",
    "model = Word2VecSkipGram(window_size=2, n=100, epochs=200, learning_rate=0.01)\n",
    "model.train(corpus)\n",
    "similar_words = model.most_similar('fox', top_n=5)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2958.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 0.999860406735583), ('lazy', 0.9983149640165497), ('jumps', 0.9975446264055375), ('brown', 0.9974962534865203), ('over', 0.9960387719803196)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Word2VecCBOW:\n",
    "    def __init__(self, window_size=2, n=100, epochs=200, learning_rate=0.01):\n",
    "        self.window_size = window_size\n",
    "        self.n = n\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def generate_training_data(self, corpus):\n",
    "        # Инициализация словаря для подсчета частоты слов\n",
    "        word_count = defaultdict(int)\n",
    "    \n",
    "        # Подсчитываем частоту каждого слова в корпусе\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                word_count[word] += 1\n",
    "    \n",
    "        # Создаем отображение слов в индексы и обратно\n",
    "        self.word2index = {word: i for i, word in enumerate(word_count.keys())}\n",
    "        self.index2word = {i: word for word, i in self.word2index.items()}\n",
    "        \n",
    "        # Определяем размер словаря (количество уникальных слов)\n",
    "        self.vocab_size = len(self.word2index)\n",
    "    \n",
    "        # Создаем список обучающих пар (context_words, target_word)\n",
    "        training_data = []\n",
    "        for sentence in corpus:\n",
    "            for i, target_word in enumerate(sentence):\n",
    "                context_words = []\n",
    "                for j in range(i - self.window_size, i + self.window_size + 1):\n",
    "                    if j != i and 0 <= j < len(sentence):\n",
    "                        context_words.append(sentence[j])\n",
    "                if context_words:\n",
    "                    training_data.append((context_words, target_word))\n",
    "    \n",
    "        return training_data\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.W1 = np.random.uniform(-1, 1, (self.vocab_size, self.n))\n",
    "        self.W2 = np.random.uniform(-1, 1, (self.n, self.vocab_size))\n",
    "\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "    def forward(self, context_words):\n",
    "        # Инициализируем входной слой (input layer) нулевым вектором\n",
    "        self.input_layer = np.zeros(self.vocab_size)\n",
    "        \n",
    "        # Вычисляем средний вектор контекстных слов путем суммирования их векторов\n",
    "        for word in context_words:\n",
    "            if word in self.word2index:\n",
    "                word_idx = self.word2index[word]\n",
    "                self.input_layer[word_idx] += 1\n",
    "        \n",
    "        # Производим вычисление скрытого слоя (hidden layer) путем умножения входного слоя на матрицу W1\n",
    "        self.hidden_layer = np.dot(self.input_layer, self.W1)\n",
    "        \n",
    "        # Производим вычисление выходного слоя (output layer) путем умножения скрытого слоя на матрицу W2\n",
    "        self.output_layer = np.dot(self.hidden_layer, self.W2)\n",
    "        \n",
    "        # Применяем softmax к выходному слою для получения вероятностей целевого слова\n",
    "        self.output_probs = self.softmax(self.output_layer)\n",
    "        \n",
    "        # Возвращаем выходные вероятности целевого слова\n",
    "        return self.output_probs\n",
    "\n",
    "    def backprop(self, context_words, target_word):\n",
    "        # Инициализируем delta_output нулевым вектором\n",
    "        delta_output = np.zeros(self.vocab_size)\n",
    "        \n",
    "        # Вычисляем ошибку на выходном слое\n",
    "        if target_word in self.word2index:\n",
    "            target_idx = self.word2index[target_word]\n",
    "            delta_output[target_idx] = 1\n",
    "        \n",
    "        # Вычисляем ошибку на скрытом слое, умножая ошибку на выходном слое на матрицу W2\n",
    "        delta_hidden = np.dot(self.W2, delta_output)\n",
    "        \n",
    "        # Обновляем матрицу W1 с учетом ошибки на скрытом слое и входного слоя\n",
    "        self.W1 -= self.learning_rate * np.outer(self.input_layer, delta_hidden)\n",
    "        \n",
    "        # Обновляем матрицу W2 с учетом ошибки на выходном слое и скрытого слоя\n",
    "        self.W2 -= self.learning_rate * np.outer(self.hidden_layer, delta_output)\n",
    "\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # Генерируем обучающие данные из предоставленного корпуса\n",
    "        training_data = self.generate_training_data(corpus)\n",
    "        \n",
    "        # Инициализируем веса модели\n",
    "        self.initialize_weights()\n",
    "    \n",
    "        # Начинаем обучение на заданном количестве эпох (self.epochs)\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            # Проходим по всем обучающим парам (context_words, target_word) в обучающих данных\n",
    "            for context_words, target_word in training_data:\n",
    "                # Выполняем прямой проход (forward pass) для текущего контекста и целевого слова\n",
    "                self.forward(context_words)\n",
    "                \n",
    "                # Выполняем обратное распространение (backpropagation) и обновляем веса модели\n",
    "                self.backprop(context_words, target_word)\n",
    "\n",
    "    def most_similar(self, word, top_n=10):\n",
    "        if word in self.word2index:\n",
    "            target_vector = self.get_word_vector(word)\n",
    "            word_sim = {}\n",
    "\n",
    "            for i in range(self.vocab_size):\n",
    "                if i != self.word2index[word]:\n",
    "                    context_word = self.index2word[i]\n",
    "                    context_vector = self.get_word_vector(context_word)\n",
    "                    cosine_similarity = np.dot(target_vector, context_vector) / (np.linalg.norm(target_vector) * np.linalg.norm(context_vector))\n",
    "                    word_sim[context_word] = cosine_similarity\n",
    "\n",
    "            return sorted(word_sim.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        if word in self.word2index:\n",
    "            target_idx = self.word2index[word]\n",
    "            return self.W1[target_idx]\n",
    "\n",
    "\n",
    "# Пример использования\n",
    "corpus = [['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']]\n",
    "model = Word2VecCBOW(window_size=2, n=100, epochs=200, learning_rate=0.01)\n",
    "model.train(corpus)\n",
    "similar_words = model.most_similar('fox', top_n=5)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
