{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgyGaRM-xNzh"
   },
   "source": [
    "https://abhinavcreed13.github.io/blog/bengio-trigram-nplm-using-pytorch/\n",
    "\n",
    "In this notebook, we are going to implement Bengio's Neural Probabilistic Language Model (NPLM) proposed in 2003 using pytorch library with GPU acceleration.\n",
    "\n",
    "Bengio's NPLM Paper: http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "We will be using brown corpus of NLTK to create this language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaQvM3ZrxKq3"
   },
   "source": [
    "## Load Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "pOdibh9vw0ac",
    "outputId": "59a01872-5d81-4b72-db3d-fae2e9232519"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\avglinsky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\avglinsky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "qkTGKGUAyDjz",
    "outputId": "44694eed-8823-43c0-8f5f-f302024156e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15667"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.paras())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxpTtEqTyIrI"
   },
   "source": [
    "We are having 15667 paragraphs in brown corpus. So, we will be using first 12000 paragraphs as the training set and rest 3000+ as the development set for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMRrTqCUybRD"
   },
   "source": [
    "## Create training and development set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2odK3Umzyldq"
   },
   "source": [
    "Using the training set, we are only adding those words in the vocabulary which are having term frequency >= 5. So, first we create vocabulary as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "djO1OiPhyeMD",
    "outputId": "85023ee6-c129-4b35-f470-9f48e710374d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12681\n"
     ]
    }
   ],
   "source": [
    "num_train = 12000\n",
    "UNK_symbol = \"<UNK>\"\n",
    "vocab = set([UNK_symbol])\n",
    "\n",
    "# create brown corpus again with all words\n",
    "# no preprocessing, only lowercase\n",
    "brown_corpus_train = []\n",
    "for idx,paragraph in enumerate(brown.paras()):\n",
    "    if idx == num_train:\n",
    "        break\n",
    "    words = []\n",
    "    for sentence in paragraph:\n",
    "        for word in sentence:\n",
    "            words.append(word.lower())\n",
    "    brown_corpus_train.append(words)\n",
    "\n",
    "# create term frequency of the words\n",
    "words_term_frequency_train = {}\n",
    "for doc in brown_corpus_train:\n",
    "    for word in doc:\n",
    "        # this will calculate term frequency\n",
    "        # since we are taking all words now\n",
    "        words_term_frequency_train[word] = words_term_frequency_train.get(word,0) + 1\n",
    "\n",
    "# create vocabulary\n",
    "for doc in brown_corpus_train:\n",
    "    for word in doc:\n",
    "        if words_term_frequency_train.get(word,0) >= 5:\n",
    "            vocab.add(word)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMCsnfLGy1lD"
   },
   "source": [
    "Now, we create training and dev set for our trigram-based NPLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "ixne4FFpzJY_",
    "outputId": "5907e0a0-fcf3-4be2-ec1d-39839dc1a4e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(872823, 2)\n",
      "(872823, 1)\n",
      "(174016, 2)\n",
      "(174016, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# create required lists\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_dev = []\n",
    "y_dev = []\n",
    "\n",
    "# create word to id mappings\n",
    "word_to_id_mappings = {}\n",
    "for idx,word in enumerate(vocab):\n",
    "    word_to_id_mappings[word] = idx\n",
    "\n",
    "# function to get id for a given word\n",
    "# return <UNK> id if not found\n",
    "def get_id_of_word(word):\n",
    "    unknown_word_id = word_to_id_mappings['<UNK>']\n",
    "    return word_to_id_mappings.get(word,unknown_word_id)\n",
    "\n",
    "# creating training and dev set\n",
    "for idx,paragraph in enumerate(brown.paras()):\n",
    "    for sentence in paragraph:\n",
    "        for i,word in enumerate(sentence):\n",
    "            if i+2 >= len(sentence):\n",
    "                # sentence boundary reached\n",
    "                # ignoring sentence less than 3 words\n",
    "                break\n",
    "            # convert word to id\n",
    "            x_extract = [get_id_of_word(word.lower()),get_id_of_word(sentence[i+1].lower())]\n",
    "            y_extract = [get_id_of_word(sentence[i+2].lower())]\n",
    "            if idx < num_train:\n",
    "                x_train.append(x_extract)\n",
    "                y_train.append(y_extract)\n",
    "            else:\n",
    "                x_dev.append(x_extract)\n",
    "                y_dev.append(y_extract)\n",
    "\n",
    "# making numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_dev = np.array(x_dev)\n",
    "y_dev = np.array(y_dev)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_dev.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn-Zy7QdzYrH"
   },
   "source": [
    "## Building Trigram NPLM with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdeupuXjzbtz"
   },
   "source": [
    "Now let's build the trigram neural language model. We'll use the language model described in Bengio's paper as:\n",
    "\n",
    "$x' = e(x_1) \\oplus e(x_2)$\n",
    "\n",
    "$h = \\tanh(W_1 x' + b)$\n",
    "\n",
    "$y = $ softmax$(W_2 h)$\n",
    "\n",
    "where $\\oplus$ is the concatenation operation, $x_1$ and $x_2$ are the input words, $e$ is an embedding function, and $y$ is the target word.\n",
    "\n",
    "We will set the dimension of the word embeddings and $h$ to 100, and train our model with 3 epochs with a batch size of 256."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu0h-XOdzyyz"
   },
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xo4a7w-vz1yE",
    "outputId": "698053b8-38bf-4344-f69d-2488fc5c8c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating training and dev dataloaders with 256 batch size ---\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import torch\n",
    "import multiprocessing\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# create parameters\n",
    "gpu = 0\n",
    "EMBEDDING_DIM = 200\n",
    "CONTEXT_SIZE = 2\n",
    "BATCH_SIZE = 256\n",
    "H = 100\n",
    "torch.manual_seed(13013)\n",
    "\n",
    "# check if gpu is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "available_workers = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"--- Creating training and dev dataloaders with {} batch size ---\".format(BATCH_SIZE))\n",
    "train_set = np.concatenate((x_train, y_train), axis=1)\n",
    "dev_set = np.concatenate((x_dev, y_dev), axis=1)\n",
    "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, num_workers = available_workers)\n",
    "dev_loader = DataLoader(dev_set, batch_size = BATCH_SIZE, num_workers = available_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCCeoPtA0Aa8"
   },
   "source": [
    "### Create pytorch Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UU0bH9vk0Cx9"
   },
   "outputs": [],
   "source": [
    "# Trigram Neural Network Model\n",
    "class TrigramNNmodel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
    "        super(TrigramNNmodel, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
    "        self.linear2 = nn.Linear(h, vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # compute x': concatenation of x1 and x2 embeddings\n",
    "        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))\n",
    "        # compute h: tanh(W_1.x' + b)\n",
    "        out = torch.tanh(self.linear1(embeds))\n",
    "        # compute W_2.h\n",
    "        out = self.linear2(out)\n",
    "        # compute y: log_softmax(W_2.h)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        # return log probabilities\n",
    "        # BATCH_SIZE x len(vocab)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnw9kysr0PDG"
   },
   "source": [
    "### Create training helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DdpYOyxd0V5E"
   },
   "outputs": [],
   "source": [
    "# helper function to get accuracy from log probabilities\n",
    "def get_accuracy_from_log_probs(log_probs, labels):\n",
    "    probs = torch.exp(log_probs)\n",
    "    predicted_label = torch.argmax(probs, dim=1)\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "# helper function to evaluate model on dev data\n",
    "def evaluate(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dev_st = time.time()\n",
    "        for it, data_tensor in enumerate(dataloader):\n",
    "            context_tensor = data_tensor[:,0:2]\n",
    "            target_tensor = data_tensor[:,2].long()\n",
    "            context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
    "            log_probs = model(context_tensor)\n",
    "            mean_loss += criterion(log_probs, target_tensor).item()\n",
    "            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "            count += 1\n",
    "            if it % 500 == 0:\n",
    "                print(\"Dev Iteration {} complete. Mean Loss: {}; Mean Acc:{}; Time taken (s): {}\".format(it, mean_loss / count, mean_acc / count, (time.time()-dev_st)))\n",
    "                dev_st = time.time()\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4g_Gdrp0csJ"
   },
   "source": [
    "### Train & Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PU8Xca760d-7",
    "outputId": "089f3f84-c231-418c-bc37-e511ae1909aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training model Epoch: 1 ---\n",
      "Training Iteration 0 of epoch 0 complete. Loss: 9.486557006835938; Acc:0.0; Time taken (s): 53.052590131759644\n",
      "Training Iteration 500 of epoch 0 complete. Loss: 6.186699867248535; Acc:0.16015625; Time taken (s): 83.95976448059082\n",
      "Training Iteration 1000 of epoch 0 complete. Loss: 6.067230701446533; Acc:0.15625; Time taken (s): 83.26248860359192\n",
      "Training Iteration 1500 of epoch 0 complete. Loss: 6.063997268676758; Acc:0.16015625; Time taken (s): 75.96039462089539\n",
      "Training Iteration 2000 of epoch 0 complete. Loss: 5.937950134277344; Acc:0.12109375; Time taken (s): 59.88210606575012\n",
      "Training Iteration 2500 of epoch 0 complete. Loss: 6.128658294677734; Acc:0.1484375; Time taken (s): 69.83636498451233\n",
      "Training Iteration 3000 of epoch 0 complete. Loss: 5.733619689941406; Acc:0.1875; Time taken (s): 65.17669439315796\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 4.984565258026123; Mean Acc:0.1796875; Time taken (s): 35.965463638305664\n",
      "Dev Iteration 500 complete. Mean Loss: 5.116284250499246; Mean Acc:0.16839757561683655; Time taken (s): 13.789291858673096\n",
      "Epoch 0 complete! Development Accuracy: 0.16767385601997375; Development Loss: 5.129401810730205\n",
      "Best development accuracy improved from 0 to 0.16767385601997375, saving model...\n",
      "\n",
      "--- Training model Epoch: 2 ---\n",
      "Training Iteration 0 of epoch 1 complete. Loss: 6.343011856079102; Acc:0.109375; Time taken (s): 37.14883279800415\n"
     ]
    }
   ],
   "source": [
    "# Using negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# create model\n",
    "model = TrigramNNmodel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
    "\n",
    "# load it to device\n",
    "model.to(device)\n",
    "\n",
    "# using ADAM optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = 2e-3)\n",
    "\n",
    "\n",
    "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
    "best_acc = 0\n",
    "best_model_path = None\n",
    "for epoch in range(5):\n",
    "    st = time.time()\n",
    "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch+1))\n",
    "    for it, data_tensor in enumerate(train_loader):\n",
    "        context_tensor = data_tensor[:,0:2]\n",
    "        target_tensor = data_tensor[:,2].long()\n",
    "\n",
    "        context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
    "\n",
    "        # zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get log probabilities over next words\n",
    "        log_probs = model(context_tensor)\n",
    "\n",
    "        # calculate current accuracy\n",
    "        acc = get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "\n",
    "        # compute loss function\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "\n",
    "        # backward pass and update gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % 500 == 0:\n",
    "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Acc:{}; Time taken (s): {}\".format(it, epoch, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "    print(\"\\n--- Evaluating model on dev data ---\")\n",
    "    dev_acc, dev_loss = evaluate(model, loss_function, dev_loader)\n",
    "    print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(epoch, dev_acc, dev_loss))\n",
    "    if dev_acc > best_acc:\n",
    "        print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "        best_acc = dev_acc\n",
    "        # set best model path\n",
    "        best_model_path = 'best_model_{}.dat'.format(epoch)\n",
    "        # saving best model\n",
    "        torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1r4VAQh08kz"
   },
   "source": [
    "## Load best model & calculate similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "t4rFvEnA1ZU6",
    "outputId": "28da1678-559b-47d3-da8c-646cde533463"
   },
   "outputs": [],
   "source": [
    "# ---------------------- Loading Best Model -------------------\n",
    "best_model = TrigramNNmodel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.to(device)\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "lm_similarities = {}\n",
    "# word pairs to calculate similarity\n",
    "words = {('computer','keyboard'),('cat','dog'),('dog','car'),('keyboard','cat')}\n",
    "\n",
    "# ----------- Calculate LM similarities using cosine similarity ----------\n",
    "for word_pairs in words:\n",
    "    w1 = word_pairs[0]\n",
    "    w2 = word_pairs[1]\n",
    "    words_tensor = torch.LongTensor([get_id_of_word(w1),get_id_of_word(w2)])\n",
    "    words_tensor = words_tensor.to(device)\n",
    "    # get word embeddings from the best model\n",
    "    words_embeds = best_model.embeddings(words_tensor)\n",
    "    # calculate cosine similarity between word vectors\n",
    "    sim = cos(words_embeds[0],words_embeds[1])\n",
    "    lm_similarities[word_pairs] = sim.item()\n",
    "\n",
    "print(lm_similarities)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
