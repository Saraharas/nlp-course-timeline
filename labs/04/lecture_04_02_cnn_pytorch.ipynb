{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3l6d1O7gbEOY"
   },
   "source": [
    "# ECS289G Project: CNN for Text Classification using PyTorch\n",
    "\n",
    "https://www.kaggle.com/code/williamlwcliu/cnn-text-classification-pytorch\n",
    "\n",
    "* A PyTorch implementation for CNN Text Classification based on [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882) (Kim, 2014).\n",
    "\n",
    "* Skeleton of this notebook is based on this post: https://chriskhanhtran.github.io/posts/cnn-sentence-classification/\n",
    "    * We are using Glove 6B 300d for pretrained embeddings, tuned hyperparameters, and extend the datasets to both MR and R8\n",
    "    * Datasets and the split strategy is from https://github.com/yao8839836/text_gcn/tree/master/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Альтернативный текст](textcnn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6mMXqnuSa-df",
    "outputId": "64a6143c-45de-45e8-8e8a-268b83f043fb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download(\"all\")\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FV5yjMaIbLuR"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Datasets\n",
    "Datasets that we will be using for our project are:\n",
    "* Movie Review(MR): http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "* R8: https://www.cs.umb.edu/˜smimarog/textmining/datasets/\n",
    "* They can also be downloaded [here](https://github.com/yao8839836/text_gcn/tree/master/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vQY6VnMHbP4P",
    "outputId": "6447ebea-1322-44ab-a22e-29e61a6520ea"
   },
   "outputs": [],
   "source": [
    "# # Download datasets\n",
    "# !rm -rf ./data\n",
    "# !wget https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/mr/text_train.txt -P ./data/mr/\n",
    "# !wget https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/mr/text_test.txt -P ./data/mr/\n",
    "# !wget https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/mr/label_train.txt -P ./data/mr/\n",
    "# !wget https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/mr/label_test.txt -P ./data/mr/\n",
    "# !wget https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/R8/train.txt -P ./data/R8/\n",
    "# !wget https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/R8/test.txt -P ./data/R8/\n",
    "\n",
    "# # Создайте папку, если она не существует\n",
    "# mkdir -Force ./data/mr/\n",
    "\n",
    "# # Скачайте файлы\n",
    "# !Invoke-WebRequest -Uri \"https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/mr/text_train.txt\" -OutFile \"./data/mr/text_train.txt\"\n",
    "# !Invoke-WebRequest -Uri \"https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/mr/text_test.txt\" -OutFile \"./data/mr/text_test.txt\"\n",
    "# !Invoke-WebRequest -Uri \"https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/mr/label_train.txt\" -OutFile \"./data/mr/label_train.txt\"\n",
    "# !Invoke-WebRequest -Uri \"https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/mr/label_test.txt\" -OutFile \"./data/mr/label_test.txt\"\n",
    "# !Invoke-WebRequest -Uri \"https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/R8/train.txt\" -OutFile \"./data/R8/train.txt\"\n",
    "# !Invoke-WebRequest -Uri \"https://raw.githubusercontent.com/yao8839836/text_gcn/master/data/R8/test.txt\" -OutFile \"./data/R8/test.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YYpB4aU3ettv"
   },
   "outputs": [],
   "source": [
    "# Preprocessing into \"list of texts\" and \"list of labels\"\n",
    "def load_mr(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        texts = []\n",
    "        for line in f:\n",
    "            item = line.decode(errors='ignore').lower().strip()\n",
    "            if item == '0' or item == '1':\n",
    "                item = int(item)\n",
    "            texts.append(item)\n",
    "\n",
    "    return np.array(texts)\n",
    "\n",
    "r8_dict = {\n",
    "    \"acq\": 0,\n",
    "    \"crude\": 1,\t\n",
    "    \"earn\": 2,\t\n",
    "    \"grain\": 3,\n",
    "    \"interest\": 4,\t\n",
    "    \"money-fx\": 5,\n",
    "    \"ship\": 6,\n",
    "    \"trade\": 7\n",
    "}\n",
    "def load_r8(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        texts = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            line = line.decode(errors='ignore').split(\"\\t\")\n",
    "            text, label = line[1], line[0]\n",
    "            texts.append(text)\n",
    "            labels.append(r8_dict[label])\n",
    "\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "mr_train_texts = load_mr('./data/mr/text_train.txt')\n",
    "mr_test_texts = load_mr('./data/mr/text_test.txt')\n",
    "mr_train_labels = load_mr('./data/mr/label_train.txt')\n",
    "mr_test_labels = load_mr('./data/mr/label_test.txt')\n",
    "r8_train_texts, r8_train_labels = load_r8('./data/R8/train.txt') \n",
    "r8_test_texts, r8_test_labels = load_r8('./data/R8/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebKqyWK1bTRr"
   },
   "source": [
    "## Pretrained Embedding and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0BveqiEDbZHG",
    "outputId": "018bec4e-d933-4953-e0a6-60744e789c20"
   },
   "outputs": [],
   "source": [
    "# Download Glove Embeddings\n",
    "URL = \"https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip\"\n",
    "FILE = \"Glove\"\n",
    "\n",
    "# if os.path.isdir(FILE):\n",
    "#     print(\"Glove exists.\")\n",
    "# else:\n",
    "#     # !wget -P $FILE $URL\n",
    "# !unzip $FILE/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oBalZE1ygjBo"
   },
   "outputs": [],
   "source": [
    "# tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "def tokenize(texts):\n",
    "    \"\"\"Tokenize texts, build vocabulary and find maximum sentence length.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of text data\n",
    "    \n",
    "    Returns:\n",
    "        tokenized_texts (List[List[str]]): List of list of tokens\n",
    "        word2idx (Dict): Vocabulary built from the corpus\n",
    "        max_len (int): Maximum sentence length\n",
    "    \"\"\"\n",
    "\n",
    "    max_len = 0\n",
    "    tokenized_texts = []\n",
    "    word2idx = {}\n",
    "\n",
    "    # Add <pad> and <unk> tokens to the vocabulary\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "\n",
    "    # Building our vocab from the corpus starting from index 2\n",
    "    idx = 2\n",
    "    for sent in texts:\n",
    "        tokenized_sent = word_tokenize(sent)\n",
    "\n",
    "        # Add `tokenized_sent` to `tokenized_texts`\n",
    "        tokenized_texts.append(tokenized_sent)\n",
    "\n",
    "        # Add new token to `word2idx`\n",
    "        for token in tokenized_sent:\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "        # Update `max_len`\n",
    "        max_len = max(max_len, len(tokenized_sent))\n",
    "\n",
    "    return tokenized_texts, word2idx, max_len\n",
    "\n",
    "def encode(tokenized_texts, word2idx, max_len):\n",
    "    \"\"\"Pad each sentence to the maximum sentence length and encode tokens to\n",
    "    their index in the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        input_ids (np.array): Array of token indexes in the vocabulary with\n",
    "            shape (N, max_len). It will the input of our CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = []\n",
    "    for tokenized_sent in tokenized_texts:\n",
    "        # Pad sentences to max_len\n",
    "        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
    "\n",
    "        # Encode tokens to input_ids\n",
    "        input_id = [word2idx.get(token) for token in tokenized_sent]\n",
    "        input_ids.append(input_id)\n",
    "    \n",
    "    return np.array(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MXiDltX5w8XW"
   },
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def load_pretrained_vectors(word2idx, fname):\n",
    "    \"\"\"Load pretrained vectors and create embedding layers.\n",
    "    \n",
    "    Args:\n",
    "        word2idx (Dict): Vocabulary built from the corpus\n",
    "        fname (str): Path to pretrained vector file\n",
    "\n",
    "    Returns:\n",
    "        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n",
    "            the size of word2idx and d is embedding dimension\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading pretrained vectors...\")\n",
    "    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    # n, d = map(int, fin.readline().split())\n",
    "    d=300\n",
    "\n",
    "    # Initilize random embeddings\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
    "    embeddings[word2idx['<pad>']] = np.zeros((d,))\n",
    "\n",
    "    # Load pretrained vectors\n",
    "    count = 0\n",
    "    for line in tqdm_notebook(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        word = tokens[0]\n",
    "        if word in word2idx:\n",
    "            count += 1\n",
    "            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
    "\n",
    "    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZgmSm_SDxDbp",
    "outputId": "979befbb-22f5-4258-8d8d-af1fa623a95a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "\n",
      "Loading pretrained vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avglinsky\\AppData\\Local\\Temp\\ipykernel_20552\\2555896567.py:27: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for line in tqdm_notebook(fin):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc8835575b143268b2fd282490e6bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17929 / 20280 pretrained vectors found.\n"
     ]
    }
   ],
   "source": [
    "train_texts = mr_train_texts\n",
    "test_texts = mr_test_texts\n",
    "# Tokenize, build vocabulary, encode tokens\n",
    "print(\"Tokenizing...\\n\")\n",
    "tokenized_texts_train, word2idx, max_len = tokenize(train_texts)\n",
    "tokenized_texts_test, word2idx, max_len = tokenize(test_texts)\n",
    "tokenized_texts, word2idx, max_len = tokenize(np.concatenate((train_texts, test_texts), axis=None))\n",
    "input_ids_train = encode(tokenized_texts_train, word2idx, max_len)\n",
    "input_ids_test = encode(tokenized_texts_test, word2idx, max_len)\n",
    "\n",
    "\n",
    "# Load pretrained vectors\n",
    "# tokenized_texts, word2idx, max_len = tokenize(np.concatenate((train_texts, test_texts), axis=None))\n",
    "embeddings = load_pretrained_vectors(word2idx, \"glove.6B.300d.txt\")\n",
    "embeddings = torch.tensor(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfizta7YxVtL"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "naCwskAyxYk_"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
    "                              SequentialSampler)\n",
    "\n",
    "def data_loader(train_inputs, test_inputs, train_labels, test_labels,\n",
    "                batch_size=50):\n",
    "    \"\"\"Convert train and validation sets to torch.Tensors and load them to\n",
    "    DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data type to torch.Tensor\n",
    "    train_inputs, test_inputs, train_labels, test_labels =\\\n",
    "    tuple(torch.tensor(data) for data in\n",
    "          [train_inputs, test_inputs, train_labels, test_labels])\n",
    "\n",
    "    # Specify batch_size\n",
    "    batch_size = batch_size\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(train_inputs, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    test_data = TensorDataset(test_inputs, test_labels)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QAQn1Sx4x33y"
   },
   "outputs": [],
   "source": [
    "# Load data to PyTorch DataLoader\n",
    "train_inputs = input_ids_train\n",
    "test_inputs = input_ids_test\n",
    "train_labels = mr_train_labels\n",
    "test_labels = mr_test_labels\n",
    "train_dataloader, test_dataloader = \\\n",
    "data_loader(train_inputs, test_inputs, train_labels, test_labels, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEiZzgf6bZ4C"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "o56wzakXbcao",
    "outputId": "5d6e7798-467a-49fd-90ad-4d2002d70a85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# set up GPU\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SjTY2yHozRxA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_NLP(nn.Module):\n",
    "    \"\"\"An 1D Convulational Neural Network for Sentence Classification.\"\"\"\n",
    "    def __init__(self,\n",
    "                 pretrained_embedding=None,\n",
    "                 freeze_embedding=False,\n",
    "                 vocab_size=None,\n",
    "                 embed_dim=300,\n",
    "                 filter_sizes=[3, 4, 5],\n",
    "                 num_filters=[100, 100, 100],\n",
    "                 num_classes=2,\n",
    "                 dropout=0.5):\n",
    "        \"\"\"\n",
    "        The constructor for CNN_NLP class.\n",
    "\n",
    "        Args:\n",
    "            pretrained_embedding (torch.Tensor): Pretrained embeddings with\n",
    "                shape (vocab_size, embed_dim)\n",
    "            freeze_embedding (bool): Set to False to fine-tune pretraiend\n",
    "                vectors. Default: False\n",
    "            vocab_size (int): Need to be specified when not pretrained word\n",
    "                embeddings are not used.\n",
    "            embed_dim (int): Dimension of word vectors. Need to be specified\n",
    "                when pretrained word embeddings are not used. Default: 300\n",
    "            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n",
    "            num_filters (List[int]): List of number of filters, has the same\n",
    "                length as `filter_sizes`. Default: [100, 100, 100]\n",
    "            n_classes (int): Number of classes. Default: 2\n",
    "            dropout (float): Dropout rate. Default: 0.5\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNN_NLP, self).__init__()\n",
    "        # Embedding layer\n",
    "        if pretrained_embedding is not None:\n",
    "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
    "                                                          freeze=freeze_embedding)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                          embedding_dim=self.embed_dim,\n",
    "                                          padding_idx=0,\n",
    "                                          max_norm=5.0)\n",
    "        # Conv Network\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=self.embed_dim,\n",
    "                      out_channels=num_filters[i],\n",
    "                      kernel_size=filter_sizes[i])\n",
    "            for i in range(len(filter_sizes))\n",
    "        ])\n",
    "        # Fully-connected layer and Dropout\n",
    "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): A tensor of token ids with shape\n",
    "                (batch_size, max_sent_length)\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Output logits with shape (batch_size,\n",
    "                n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n",
    "        x_embed = self.embedding(input_ids).float()\n",
    "\n",
    "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
    "        # Output shape: (b, embed_dim, max_len)\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\n",
    "\n",
    "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
    "\n",
    "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
    "            for x_conv in x_conv_list]\n",
    "        \n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\n",
    "        # Output shape: (b, sum(num_filters))\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
    "                         dim=1)\n",
    "        \n",
    "        # Compute logits. Output shape: (b, n_classes)\n",
    "        logits = self.fc(self.dropout(x_fc))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0YU6Jx-HzVn3"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def initilize_model(pretrained_embedding=None,\n",
    "                    freeze_embedding=False,\n",
    "                    vocab_size=None,\n",
    "                    embed_dim=300,\n",
    "                    filter_sizes=[3, 4, 5],\n",
    "                    num_filters=[100, 100, 100],\n",
    "                    num_classes=2,\n",
    "                    dropout=0.5,\n",
    "                    learning_rate=0.01,\n",
    "                    weight_decay=0):\n",
    "    \"\"\"Instantiate a CNN model and an optimizer.\"\"\"\n",
    "\n",
    "    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and \\\n",
    "    num_filters need to be of the same length.\"\n",
    "\n",
    "    # Instantiate CNN model\n",
    "    cnn_model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n",
    "                        freeze_embedding=freeze_embedding,\n",
    "                        vocab_size=vocab_size,\n",
    "                        embed_dim=embed_dim,\n",
    "                        filter_sizes=filter_sizes,\n",
    "                        num_filters=num_filters,\n",
    "                        num_classes=num_classes,\n",
    "                        dropout=0.5)\n",
    "    \n",
    "    # Send model to `device` (GPU/CPU)\n",
    "    cnn_model.to(device)\n",
    "\n",
    "    # Instantiate Adadelta optimizer\n",
    "    optimizer = optim.Adadelta(cnn_model.parameters(),\n",
    "                               lr=learning_rate,\n",
    "                               rho=0.95,\n",
    "                               weight_decay=weight_decay)\n",
    "\n",
    "    return cnn_model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4xSA45kbd7b"
   },
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jrP2hQmdbgwx"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, optimizer, train_dataloader, test_dataloader=None, epochs=10, model_name=\"\"):\n",
    "    \"\"\"Train the CNN model.\"\"\"\n",
    "    \n",
    "    # Tracking best validation accuracy\n",
    "    best_accuracy = 0\n",
    "    train_time = 0\n",
    "\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Test Loss':^10} | {'Test Acc':^9} | {'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "\n",
    "        # Tracking time and loss\n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels.to(torch.long))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "        train_time += time.time() - t0_epoch\n",
    "\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if test_dataloader is not None:\n",
    "            # After the completion of each training epoch, measure the model's\n",
    "            # performance on our validation set.\n",
    "            test_loss, test_accuracy = evaluate(model, test_dataloader)\n",
    "\n",
    "            # Track the best accuracy\n",
    "            if test_accuracy > best_accuracy:\n",
    "                best_accuracy = test_accuracy\n",
    "                torch.save(model, \"./models/\" + model_name + \"_best_model.pt\")\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {test_loss:^10.6f} | {test_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            \n",
    "    print(\"\\n\")\n",
    "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
    "    return best_accuracy, train_time\n",
    "\n",
    "def evaluate(model, test_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's\n",
    "    performance on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled\n",
    "    # during the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    test_accuracy = []\n",
    "    test_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels.to(torch.long))\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        test_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    test_loss = np.mean(test_loss)\n",
    "    test_accuracy = np.mean(test_accuracy)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "I0m1ocTtdipj"
   },
   "outputs": [],
   "source": [
    "# !mkdir ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters count\n",
    "def count_params(model):\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    pytorch_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Total Parameters: \" + str(pytorch_total_params))\n",
    "    print(\"Trainable Parameters: \" + str(pytorch_trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal std and mean of list\n",
    "def mean_std(arr):\n",
    "    arr = np.array(arr)\n",
    "    mean = np.mean(arr)\n",
    "    std = np.std(arr)\n",
    "    return mean, std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qLMj1Vk2bkEg",
    "outputId": "c99e5bc6-5e51-42ce-ebae-7975688744b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 6444902\n",
      "Trainable Parameters: 6444902\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  | Test Loss  | Test Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   0.683541   |  0.674780  |   59.07   |   24.12  \n",
      "   2    |   0.616908   |  0.638561  |   62.33   |   24.76  \n",
      "   3    |   0.546231   |  0.625204  |   65.31   |   23.25  \n",
      "   4    |   0.453418   |  0.625068  |   67.18   |   23.24  \n",
      "   5    |   0.381031   |  0.596784  |   68.32   |   23.13  \n",
      "   6    |   0.286137   |  0.613099  |   68.40   |   23.17  \n",
      "   7    |   0.224490   |  0.673897  |   66.22   |   23.20  \n",
      "   8    |   0.173115   |  0.615674  |   70.61   |   23.22  \n",
      "   9    |   0.130919   |  0.621310  |   71.33   |   23.42  \n",
      "  10    |   0.108123   |  0.734275  |   67.89   |   25.61  \n",
      "  11    |   0.081979   |  0.646775  |   70.99   |   25.22  \n",
      "  12    |   0.075149   |  0.740768  |   67.75   |   23.61  \n",
      "  13    |   0.068093   |  0.652815  |   71.21   |   24.66  \n",
      "  14    |   0.061267   |  0.755015  |   67.93   |   24.06  \n",
      "  15    |   0.055112   |  0.628129  |   72.22   |   24.35  \n",
      "  16    |   0.048982   |  0.708712  |   70.92   |   26.88  \n",
      "  17    |   0.049971   |  0.690451  |   70.62   |   25.33  \n",
      "  18    |   0.048896   |  0.617173  |   73.11   |   25.97  \n",
      "  19    |   0.048292   |  0.606007  |   73.10   |   22.88  \n",
      "  20    |   0.048807   |  0.610472  |   72.40   |   23.37  \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 73.11%.\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  | Test Loss  | Test Acc  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   0.048673   |  0.593831  |   74.03   |   24.24  \n",
      "   2    |   0.049885   |  0.633949  |   71.76   |   23.97  \n",
      "   3    |   0.050357   |  0.663067  |   71.97   |   24.00  \n",
      "   4    |   0.049387   |  0.607969  |   73.24   |   23.90  \n",
      "   5    |   0.050369   |  0.629392  |   72.90   |   23.93  \n",
      "   6    |   0.050788   |  0.599133  |   74.43   |   23.88  \n",
      "   7    |   0.052513   |  0.644724  |   73.32   |   23.95  \n",
      "   8    |   0.049379   |  0.597721  |   74.60   |   24.31  \n",
      "   9    |   0.052264   |  0.660200  |   72.71   |   25.77  \n",
      "  10    |   0.050931   |  0.600746  |   74.57   |   25.94  \n",
      "  11    |   0.049612   |  0.607038  |   74.29   |   27.10  \n",
      "  12    |   0.048053   |  0.629948  |   73.93   |   25.72  \n",
      "  13    |   0.047627   |  0.623855  |   74.29   |   27.14  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m train_t \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m     best_acc, train_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_rand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmr_cnn_rand\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     acc\u001b[38;5;241m.\u001b[39mappend(best_acc)\n\u001b[0;32m     15\u001b[0m     train_t\u001b[38;5;241m.\u001b[39mappend(train_time)\n",
      "Cell \u001b[1;32mIn[13], line 57\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_dataloader, test_dataloader, epochs, model_name)\u001b[0m\n\u001b[0;32m     54\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Calculate the average loss over the entire training data\u001b[39;00m\n\u001b[0;32m     60\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n",
      "File \u001b[1;32m~\\PycharmProjects\\nlp-course-timeline\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\nlp-course-timeline\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\PycharmProjects\\nlp-course-timeline\\venv\\Lib\\site-packages\\torch\\optim\\adadelta.py:108\u001b[0m, in \u001b[0;36mAdadelta.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     96\u001b[0m     lr, rho, eps, weight_decay, foreach, maximize, differentiable \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     97\u001b[0m         group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     98\u001b[0m         group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrho\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m         group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    104\u001b[0m     )\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, grads, square_avgs, acc_deltas)\n\u001b[1;32m--> 108\u001b[0m     \u001b[43madadelta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43msquare_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43macc_deltas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforeach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\PycharmProjects\\nlp-course-timeline\\venv\\Lib\\site-packages\\torch\\optim\\adadelta.py:206\u001b[0m, in \u001b[0;36madadelta\u001b[1;34m(params, grads, square_avgs, acc_deltas, foreach, differentiable, lr, rho, eps, weight_decay, maximize)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adadelta\n\u001b[1;32m--> 206\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43msquare_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43macc_deltas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\nlp-course-timeline\\venv\\Lib\\site-packages\\torch\\optim\\adadelta.py:249\u001b[0m, in \u001b[0;36m_single_tensor_adadelta\u001b[1;34m(params, grads, square_avgs, acc_deltas, lr, rho, eps, weight_decay, maximize, differentiable)\u001b[0m\n\u001b[0;32m    247\u001b[0m square_avg\u001b[38;5;241m.\u001b[39mmul_(rho)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m rho)\n\u001b[0;32m    248\u001b[0m std \u001b[38;5;241m=\u001b[39m square_avg\u001b[38;5;241m.\u001b[39madd(eps)\u001b[38;5;241m.\u001b[39msqrt_()\n\u001b[1;32m--> 249\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[43macc_delta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqrt_()\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n\u001b[0;32m    251\u001b[0m     delta \u001b[38;5;241m=\u001b[39m delta\u001b[38;5;241m.\u001b[39mclone()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# need to create dir: \"./models\"\n",
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "# set_seed(42)\n",
    "cnn_rand, optimizer = initilize_model(vocab_size=len(word2idx),\n",
    "                                      embed_dim=300,\n",
    "                                      learning_rate=0.5,\n",
    "                                      dropout=0.5,\n",
    "                                      weight_decay=1e-3)\n",
    "count_params(cnn_rand)\n",
    "acc = []\n",
    "train_t = []\n",
    "for i in range(10):\n",
    "    best_acc, train_time = train(cnn_rand, optimizer, train_dataloader, test_dataloader, epochs=20, model_name=\"mr_cnn_rand\")\n",
    "    acc.append(best_acc)\n",
    "    train_t.append(train_time)\n",
    "\n",
    "# cal avg and std of acc and time\n",
    "acc_mean, acc_std = mean_std(acc)\n",
    "t_mean, t_std = mean_std(train_t)\n",
    "print(\"Average accuracy: \" + str(acc_mean) + \" Acc std: \" + str(acc_std))\n",
    "print(\"Average time: \" + str(t_mean) + \" time std: \" + str(t_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOTxf4khzlkb",
    "outputId": "cf2529ea-82c0-433e-ad2d-b68b49775aa2"
   },
   "outputs": [],
   "source": [
    "# CNN-static: pretrained word vectors are used and freezed during training.\n",
    "# set_seed(42)\n",
    "cnn_static, optimizer = initilize_model(pretrained_embedding=embeddings,\n",
    "                                        freeze_embedding=True,\n",
    "                                        learning_rate=0.5,\n",
    "                                        dropout=0.5,\n",
    "                                        weight_decay=1e-3)\n",
    "count_params(cnn_static)\n",
    "acc = []\n",
    "train_t = []\n",
    "for i in range(10):\n",
    "    best_acc, train_time = train(cnn_static, optimizer, train_dataloader, test_dataloader, epochs=20, model_name=\"mr_cnn_static\")\n",
    "    acc.append(best_acc)\n",
    "    train_t.append(train_time)\n",
    "    \n",
    "# cal avg and std of acc and time\n",
    "acc_mean, acc_std = mean_std(acc)\n",
    "t_mean, t_std = mean_std(train_t)\n",
    "print(\"Average accuracy: \" + str(acc_mean) + \" Acc std: \" + str(acc_std))\n",
    "print(\"Average time: \" + str(t_mean) + \" time std: \" + str(t_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_o_p7JwYznlC",
    "outputId": "c81a2086-9f92-4952-b2fb-d74563b637be"
   },
   "outputs": [],
   "source": [
    "# CNN-non-static: pretrained word vectors are fine-tuned during training.\n",
    "# set_seed(42)\n",
    "cnn_non_static, optimizer = initilize_model(pretrained_embedding=embeddings,\n",
    "                                            freeze_embedding=False,\n",
    "                                            learning_rate=0.5,\n",
    "                                            dropout=0.5,\n",
    "                                            weight_decay=1e-3)\n",
    "count_params(cnn_non_static)\n",
    "acc = []\n",
    "train_t = []\n",
    "for i in range(10):\n",
    "    best_acc, train_time = train(cnn_non_static, optimizer, train_dataloader, test_dataloader, epochs=20, model_name=\"mr_cnn_non_static\")\n",
    "    acc.append(best_acc)\n",
    "    train_t.append(train_time)\n",
    "\n",
    "# cal avg and std of acc and time\n",
    "acc_mean, acc_std = mean_std(acc)\n",
    "t_mean, t_std = mean_std(train_t)\n",
    "print(\"Average accuracy: \" + str(acc_mean) + \" Acc std: \" + str(acc_std))\n",
    "print(\"Average time: \" + str(t_mean) + \" time std: \" + str(t_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtBkHiLujY1O"
   },
   "source": [
    "## R8 Dataset Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NB5Pltjjdyq",
    "outputId": "c7b6c854-f8de-4bf1-f39c-2e3be2a49ec7"
   },
   "outputs": [],
   "source": [
    "train_texts = r8_train_texts\n",
    "test_texts = r8_test_texts\n",
    "# Tokenize, build vocabulary, encode tokens\n",
    "print(\"Tokenizing...\\n\")\n",
    "tokenized_texts_train, word2idx, max_len = tokenize(train_texts)\n",
    "tokenized_texts_test, word2idx, max_len = tokenize(test_texts)\n",
    "tokenized_texts, word2idx, max_len = tokenize(np.concatenate((train_texts, test_texts), axis=None))\n",
    "input_ids_train = encode(tokenized_texts_train, word2idx, max_len)\n",
    "input_ids_test = encode(tokenized_texts_test, word2idx, max_len)\n",
    "\n",
    "\n",
    "# Load pretrained vectors\n",
    "# tokenized_texts, word2idx, max_len = tokenize(np.concatenate((train_texts, test_texts), axis=None))\n",
    "embeddings = load_pretrained_vectors(word2idx, \"glove.6B.300d.txt\")\n",
    "embeddings = torch.tensor(embeddings)\n",
    "\n",
    "# Load data to PyTorch DataLoader\n",
    "train_inputs = input_ids_train\n",
    "test_inputs = input_ids_test\n",
    "train_labels = r8_train_labels\n",
    "test_labels = r8_test_labels\n",
    "train_dataloader, test_dataloader = \\\n",
    "data_loader(train_inputs, test_inputs, train_labels, test_labels, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "st8iJiZ4kB5T",
    "outputId": "7b3dfc02-b372-4efa-a0fa-d11d87a93537"
   },
   "outputs": [],
   "source": [
    "# CNN-rand: Word vectors are randomly initialized.\n",
    "# set_seed(42)\n",
    "cnn_rand, optimizer = initilize_model(vocab_size=len(word2idx),\n",
    "                                      embed_dim=300,\n",
    "                                      learning_rate=0.5,\n",
    "                                      dropout=0.5,\n",
    "                                      num_classes=8,\n",
    "                                      weight_decay=1e-3)\n",
    "count_params(cnn_rand)\n",
    "acc = []\n",
    "train_t = []\n",
    "for i in range(10):\n",
    "    best_acc, train_time = train(cnn_rand, optimizer, train_dataloader, test_dataloader, epochs=20, model_name=\"r8_cnn_rand\")\n",
    "    acc.append(best_acc)\n",
    "    train_t.append(train_time)\n",
    "\n",
    "# cal avg and std of acc and time\n",
    "acc_mean, acc_std = mean_std(acc)\n",
    "t_mean, t_std = mean_std(train_t)\n",
    "print(\"Average accuracy: \" + str(acc_mean) + \" Acc std: \" + str(acc_std))\n",
    "print(\"Average time: \" + str(t_mean) + \" time std: \" + str(t_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jG7aZ4ahkCDL",
    "outputId": "32e2811f-f80b-4f89-a93b-d2d828fd80af"
   },
   "outputs": [],
   "source": [
    "# CNN-static: pretrained word vectors are used and freezed during training.\n",
    "# set_seed(42)\n",
    "cnn_static, optimizer = initilize_model(pretrained_embedding=embeddings,\n",
    "                                        freeze_embedding=True,\n",
    "                                        learning_rate=0.5,\n",
    "                                        dropout=0.5,\n",
    "                                        num_classes=8,\n",
    "                                        weight_decay=1e-3)\n",
    "count_params(cnn_static)\n",
    "acc = []\n",
    "train_t = []\n",
    "for i in range(10):\n",
    "    best_acc, train_time = train(cnn_static, optimizer, train_dataloader, test_dataloader, epochs=20, model_name=\"r8_cnn_static\")\n",
    "    acc.append(best_acc)\n",
    "    train_t.append(train_time)\n",
    "\n",
    "# cal avg and std of acc and time\n",
    "acc_mean, acc_std = mean_std(acc)\n",
    "t_mean, t_std = mean_std(train_t)\n",
    "print(\"Average accuracy: \" + str(acc_mean) + \" Acc std: \" + str(acc_std))\n",
    "print(\"Average time: \" + str(t_mean) + \" time std: \" + str(t_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEA8uht7kCIX"
   },
   "outputs": [],
   "source": [
    "# CNN-non-static: pretrained word vectors are fine-tuned during training.\n",
    "# set_seed(42)\n",
    "cnn_non_static, optimizer = initilize_model(pretrained_embedding=embeddings,\n",
    "                                            freeze_embedding=False,\n",
    "                                            learning_rate=0.5,\n",
    "                                            dropout=0.5,\n",
    "                                            num_classes=8,\n",
    "                                            weight_decay=1e-3)\n",
    "count_params(cnn_non_static)\n",
    "acc = []\n",
    "train_t = []\n",
    "for i in range(10):\n",
    "    best_acc, train_time = train(cnn_non_static, optimizer, train_dataloader, test_dataloader, epochs=20, model_name=\"r8_cnn_non_static\")\n",
    "    acc.append(best_acc)\n",
    "    train_t.append(train_time)\n",
    "    \n",
    "# cal avg and std of acc and time\n",
    "acc_mean, acc_std = mean_std(acc)\n",
    "t_mean, t_std = mean_std(train_t)\n",
    "print(\"Average accuracy: \" + str(acc_mean) + \" Acc std: \" + str(acc_std))\n",
    "print(\"Average time: \" + str(t_mean) + \" time std: \" + str(t_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAQYAze5paRB"
   },
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAZ-pBx-pdbo"
   },
   "outputs": [],
   "source": [
    "def predict_review(text, model=cnn_non_static.to(\"cpu\"), max_len=62):\n",
    "    \"\"\"Predict probability that a review is positive.\"\"\"\n",
    "\n",
    "    # Tokenize, pad and encode text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    padded_tokens = tokens + ['<pad>'] * (max_len - len(tokens))\n",
    "    input_id = [word2idx.get(token, word2idx['<unk>']) for token in padded_tokens]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n",
    "\n",
    "    # Compute logits\n",
    "    logits = model.forward(input_id)\n",
    "\n",
    "    #  Compute probability\n",
    "    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n",
    "\n",
    "    print(f\"This review is {probs[1] * 100:.2f}% positive.\")\n",
    "\n",
    "r8_id2cat = {\n",
    "    \"acq\": 0,\n",
    "    \"crude\": 1,\t\n",
    "    \"earn\": 2,\t\n",
    "    \"grain\": 3,\n",
    "    \"interest\": 4,\t\n",
    "    \"money-fx\": 5,\n",
    "    \"ship\": 6,\n",
    "    \"trade\": 7\n",
    "}\n",
    "\n",
    "def predict_r8(text, model=cnn_non_static.to(\"cpu\"), max_len=62):\n",
    "    \"\"\"Predict probability of each category in r8.\"\"\"\n",
    "\n",
    "    # Tokenize, pad and encode text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    padded_tokens = tokens + ['<pad>'] * (max_len - len(tokens))\n",
    "    input_id = [word2idx.get(token, word2idx['<unk>']) for token in padded_tokens]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n",
    "\n",
    "    # Compute logits\n",
    "    logits = model.forward(input_id)\n",
    "\n",
    "    #  Compute probability\n",
    "    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n",
    "\n",
    "    for i in range(8):\n",
    "        print(f\"This review is {probs[i] * 100:.2f}% {r8_id2cat[i]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fr6SDbqYq2Hh"
   },
   "outputs": [],
   "source": [
    "test_model = torch.load(\"./models/mr_cnn_non_static_best_model.pt\")\n",
    "test_model.to(\"cpu\")\n",
    "test_model.eval()\n",
    "predict_review(\"All of friends slept while watching this movie. But I really enjoyed it.\", model=test_model)\n",
    "predict_review(\"I have waited so long for this movie. I am now so satisfied and happy.\", model=test_model)\n",
    "predict_review(\"This movie is long and boring.\", model=test_model)\n",
    "predict_review(\"I don't like the ending.\", model=test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imapct of Document Length(R8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_text(input_id):\n",
    "    cnt = 0\n",
    "    for token in input_id:\n",
    "        if token != 0:\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "def evaluate_doc_length(model, test_dataloader):\n",
    "    \"\"\"\n",
    "    we divide documents into extreme short (less than 30 words), \n",
    "    short(30-50 words), medium (50-70 words), and long (more than 70 words)..\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled\n",
    "    # during the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    c_es = 0\n",
    "    c_s = 0\n",
    "    c_m = 0\n",
    "    c_l = 0\n",
    "    t_es = 0\n",
    "    t_s = 0\n",
    "    t_m = 0\n",
    "    t_l = 0\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        for input_id, label in zip(b_input_ids, b_labels):\n",
    "\n",
    "            # Compute logits\n",
    "            length = len_text(input_id)\n",
    "            with torch.no_grad():\n",
    "                input_id = input_id.clone().detach().unsqueeze(dim=0)\n",
    "                logit = model(input_id)\n",
    "\n",
    "            # Get the predictions\n",
    "            pred = torch.argmax(logit)\n",
    "\n",
    "            # Calculate the accuracy rate\n",
    "            if length > 70:\n",
    "                t_l += 1\n",
    "                if pred == label:\n",
    "                    c_l += 1\n",
    "            elif length >= 50:\n",
    "                t_m += 1\n",
    "                if pred == label:\n",
    "                    c_m += 1\n",
    "            elif length >= 30:\n",
    "                t_s += 1\n",
    "                if pred == label:\n",
    "                    c_s += 1\n",
    "            else:\n",
    "                t_es += 1\n",
    "                if pred == label:\n",
    "                    c_es += 1\n",
    "                \n",
    "\n",
    "    # Compute acc\n",
    "    print(\"Acc of extreme short: \" + str(c_es/t_es))\n",
    "    print(\"Acc of short: \" + str(c_s/t_s))\n",
    "    print(\"Acc of medium: \" + str(c_m/t_m))\n",
    "    print(\"Acc of long: \" + str(c_l/t_l))\n",
    "    \n",
    "evaluate_doc_length(cnn_static, test_dataloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U25M9LGrbl9e"
   },
   "source": [
    "## Conclusion\n",
    "* CNN networks looks effective on text classification problem, it's competitive to other networks like LSTM, GNN, etc.\n",
    "* We reached around 77-78% testing accuracy on Movie Review dataset, around 97% testing accuracy on R8 dataset, with a really fast training speed\n",
    "* We tried with several hyperparameters but we did not conduct any hyperparameters search, however, the accuracy looks good\n",
    "* Glove pretrained embeddings definitely helps, but the non-static embeddings doesn't improve much"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
